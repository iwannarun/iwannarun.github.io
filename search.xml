<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[如何通过自增ID生成可逆短码(链接)]]></title>
    <url>%2F2017%2F11%2F03%2F%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E8%87%AA%E5%A2%9EID%E7%94%9F%E6%88%90%E5%8F%AF%E9%80%86%E7%9F%AD%E7%A0%81(%E9%93%BE%E6%8E%A5)%2F</url>
    <content type="text"><![CDATA[最近有一个生成短链接(地址、标识码)的需求，需要将数据库自增ID通过策略转换或加密成一串编码，要求生成的编码不应过长，不易破解并且可随意解析出原ID。或者说，这个需求有点类似微博站点短地址(https://t.cn/x2ha4)访问后会转变成一串真实长度的地址。 思路 现成的可逆加密方式如AES256、DES等生成密码串过长，不符合需求(压缩也还是太长)。 MD5等单向加密方式，需要维护映射关系。每次访问需要查库在大数据量下影响像你，并且有一定几率发生碰撞(彩虹表)。 Base62加密，由26个小写字母+26个大写字母+10个数字共62个字符组成。将自增ID的十进制转为62进制，实现转码，并且可逆。 方案 通过Base62进行编码的方式可以得到相对比较精简的短码，这里虽然达到加密和压缩的效果却忽视自增ID带来的问题，Base62加密后的短码保留了”自增”的特性，容易被反解出前后编码。 最终方案查阅资料发现一种广泛应用于密码学的”Feistel算法”,具有非常奇妙的特性，那就是，在一定的数字范围内（2的n次方），每一个数字都能根据密钥找到唯一的一个匹配对象。即: 如果permutedId(a)=b，那么必然会有permutedId(b)=a。 利用这个特点可轻松移除ID自增的属性，将ID转为另一个与之对应的”密码”，最后通过Base62转换得到最终符合我们的需求短码。 代码package com.xy.poker.util; import java.util.Stack; /** * 将自增id转成短链接, * 同时短连接可逆回自增id, * 相邻自增id无规律，防破解。 * 基于Feistel的特性转为Base62。 * * @author run * @create 2017-11-02 **/ public class ShortCodeKit { /** * Feistel密码结构：如果permutedId(a)=b，那么必然会有permutedId(b)=a * * @param id * @return */ public static Long permutedId(Long id) { Long l1 = (id &gt;&gt; 16) &amp; 65535; Long r1 = id &amp; 65535; for (int i = 0; i &lt; 2; i++) { Long l2 = r1; Long r2 = l1 ^ (int) (roundFunction(r1) * 65535); l1 = l2; r1 = r2; } return ((r1 &lt;&lt; 16) + l1); } public static Double roundFunction(Long val) { return ((131239 * val + 15534) % 714025) / 714025.0; } /** * 调整字符顺序可增加混淆 */ private static char[] charSet = &quot;0WqPQRI7yCDE31VONvSXnxY2bcdJK8zoLMZa9AmBjklpTUu45FGrst6Hwefghi&quot;.toCharArray(); /** * 将10进制转化为62进制 * * @param number * @param length 转化成的62进制长度，不足length长度的话高位补0，否则不改变什么 * @return */ public static String convertDecimalToBase62(long number, int length) { Long rest = number; Stack&lt;Character&gt; stack = new Stack&lt;Character&gt;(); StringBuilder result = new StringBuilder(0); while (rest != 0) { stack.add(charSet[new Long((rest - (rest / 62) * 62)).intValue()]); rest = rest / 62; } for (; !stack.isEmpty(); ) { result.append(stack.pop()); } int resultLength = result.length(); StringBuilder temp0 = new StringBuilder(); for (int i = 0; i &lt; length - resultLength; i++) { temp0.append(&apos;0&apos;); } return temp0.toString() + result.toString(); } /** * 将62进制转换成10进制数 * * @param str * @return */ public static String convertBase62ToDecimal(String str) { int multiple = 1; long result = 0; Character c; for (int i = 0; i &lt; str.length(); i++) { c = str.charAt(str.length() - i - 1); result += getCharValue(c) * multiple; multiple = multiple * 62; } return result + &quot;&quot;; } /** * 获取字符下标 * @param c * @return */ private static int getCharValue(Character c) { for (int i = 0; i &lt; charSet.length; i++) { if (c == charSet[i]) { return i; } } return -1; } public static void main(String[] args) { //转码测试 Long id = permutedId(10230L); Long oid = permutedId(id); System.out.println(id); System.out.println(oid); String s = convertDecimalToBase62(id, 8); System.out.println(s); String r = convertBase62ToDecimal(s); System.out.println(r); System.out.println(permutedId(Long.parseLong(r))); //相邻转码无关性测试 for (int i = 10000; i &lt; 40000; i++) { Long newId = permutedId(Long.valueOf(i)); Long de = permutedId(newId); String code = ShortCodeKit.convertDecimalToBase62(newId, 8); String preCode = convertBase62ToDecimal(code); System.out.println(String.format(&quot;原ID=%d, 一次permuted的ID=%d, 二次permuted后的ID=%d, Base62加密后=%s,解密=%d&quot;, i, newId, de, code, permutedId(Long.parseLong(preCode)))); } } }]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你看那个人好像一条狗哦]]></title>
    <url>%2F2017%2F10%2F20%2F%E4%BD%A0%E7%9C%8B%E9%82%A3%E4%B8%AA%E4%BA%BA%E5%A5%BD%E5%83%8F%E4%B8%80%E6%9D%A1%E7%8B%97%E5%93%A6%2F</url>
    <content type="text"><![CDATA[]]></content>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你好，Hexo]]></title>
    <url>%2F2017%2F10%2F01%2FMy-New-Post%2F</url>
    <content type="text"><![CDATA[首先，这是在Hexo的第一篇博客，那么就先来说说选择Hexo的理由。 由于自己懒癌，原先的Ghost这几个月都没更新了。并非不想记录，而是每次想记点东西的时候想到Ghost写个博客需要先登录博客后台(手动输入博客域名/ghost)，接着输入账号密码(浏览器通常没存)，写的时候又要在编辑器上调整格式，发布后预览检查主题带来的显示差异…想到这总嫌麻烦就不写了。 现在有了Hexo，记录博客的方式发生了质的改变。 创建一篇新的文章，本地写好MD文件，保存一下后就是类似提交文件到预置的git仓库的操作，hexo deploy一下就自动发布到gitpage了，这样的一套操作下来真的是如丝顺滑，颇有持续集成的感觉。 至于这款博客其他的功能体验，目前单博客的配置方式就足够吸引我了。Hexo以.yml语法进行站点配置对于程序员来说是友好的。gayhub.com上前端大大们开源的主题看了一圈，大都还挺有新意的，可玩性会比Ghost高些。其他部分还在体验当中。 So，后续会把阿里云机器上ghost博客迁移到这里来，想想每个月省下的主机费用还可以多几买本书或者女票的口红，嗯乐滋滋。]]></content>
      <tags>
        <tag>文章</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编程规范:泛型的使用]]></title>
    <url>%2F2017%2F09%2F08%2Fpecs-producer-extends-consumer-super-de-yi-si%2F</url>
    <content type="text"><![CDATA[遵守PECS(Producer Extends Consumer Super)原则 解释 频繁往外读取内容的，适合用上界 Extends。 经常往里插入的，适合用下界 Super。 应用Java中，泛型通配符&lt;? extends T&gt;来接收返回的数据，此写法的泛型集合不能使用add方 法，而&lt;? super T&gt;不能使用get方法，做为接口调用赋值时易出错。 举例 &lt;? extends T&gt;, ? 必须是T或T的子类集合写(add)： 因为不能确定集合实例化时用的是T或T的子类，所以没有办法写。例如： List&lt;? extends Number&gt; foo = new ArrayList&lt;Number/Integer/Double&gt;()，你不能add Number，因为也可能是Integer或Double的List， 同理也不能add Integer或Double，即，extends T， 不能集合add。集合读(get)： 只能读出T类型的数据。&lt;? super T&gt;， ? 必须是T或T的父类集合写(add)： 可以add T或T的子类。集合读(get)： 不能确定从集合里读出的是哪个类型(可能是T也可能是T的父类，或者Object)，所以没有办法使用get。例如：List&lt;? super Integer&gt; foo3 = new ArrayList&lt;Integer/Number/Object&gt;(); 只能保证get出来是Object。 下面是示例，test1和test2在编译时都有错误提示。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.robert.javaspec; import java.util.LinkedList; import java.util.List; /** * Created by WangMeng on 2017-04-13. * FIX ME */ public class Main &#123; public static void main(String[] args) &#123; &#125; public void test1()&#123; List&lt;? extends A&gt; childofa=new LinkedList&lt;&gt;(); B b=new B(); A a=new A(); childofa.add(a); childofa.add(b); A ta= childofa.get(0); &#125; public void test2()&#123; List&lt;? super B&gt; superOfb = new LinkedList&lt;&gt;(); B b = new B(); A a = new A(); superOfb.add(a); superOfb.add(b); A ta = superOfb.get(0); B tb = superOfb.get(0); &#125; &#125; class A &#123; @Override public String toString() &#123; return &quot;A&quot;; &#125; &#125; class B extends A &#123; @Override public String toString() &#123; return &quot;B&quot;; &#125; &#125; 转自https://yq.aliyun.com/articles/181420?spm=5176.100240.searchblog.35.CUGecC]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一段批量插入sql脚本]]></title>
    <url>%2F2017%2F08%2F03%2Fyi-duan-pi-liang-cha-ru-sql%2F</url>
    <content type="text"><![CDATA[123456789101112#!/bin/bashi=1;MAX_INSERT_ROW_COUNT=$1;while [ $i -le $MAX_INSERT_ROW_COUNT ]do echo &quot;INSERT INTO player (os,username,password,nickname,emoney,dataComplete,roomId,clubId,lastOnlineTime,diamond,personalizedSignature,sex,imgUrl,codeCheck,code,codeLastTime,vipType,vipValid,inSss,countryCode,inSng,isAdmin,speak_ban,ip) VALUES (&apos;1&apos;, &apos;44444$i&apos;, &apos;123456&apos;, &apos;TEST$i&apos;, &apos;100000&apos;, &apos;1&apos;, &apos;0&apos;, &apos;0&apos;, &apos;1473237255&apos;, &apos;10000&apos;, &apos;&apos;, &apos;1&apos;, &apos;&apos;, &apos;1&apos;, &apos;834162&apos;, &apos;1473237255&apos;, &apos;0&apos;, &apos;0&apos;, &apos;0&apos;, &apos;86&apos;, &apos;0&apos;, &apos;0&apos;, &apos;0&apos;, &apos;&apos;);&quot; &gt;&gt; player.sql d=$(date +%M-%d\ %H\:%m\:%S) echo &quot;INSERT HELLO $i @@ $d&quot; i=$(($i+1))doneexit 0]]></content>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Web常见攻击手段和维护]]></title>
    <url>%2F2017%2F06%2F17%2Fweb%2F</url>
    <content type="text"><![CDATA[一、SQL注入攻击 解决方案： 前端页面校验用户的输入数据（限制用户输入的类型、范围、格式、长度等）。 后端对提交的数据进行严格校验，数据存储时不要使用“+”号拼接sql，采用预编译sql，防止注入！ 使用自定义错误页，防止用户通过服务器默认的错误页面找到服务器漏洞。 二、XSS跨站脚本攻击 攻击方式：基于DOM的XSS即通过浏览器来直接运行js脚本，无须提交服务器，从客户端的代码引起的。 如：其实就是发送一个合法的地址加自己的脚本，比如： 1www.xxx.com/search?wd=&lt;script&gt;...&lt;/script&gt; 受害者点击的是 1www.xxx.com/search?wd=&lt;script&gt;...&lt;/script&gt; 链接，然后受害者的浏览网页就加入这个恶意代码。存储XSS攻击即通过输入框提交js脚本或者上传文件到服务器，从网站的数据库引起的攻击。反射XSS攻击即通过url提交js脚本到服务器，从受害人的请求发起引起的攻击。 目的：盗取用户的登陆账户密码，收集用户的cookie等敏感信息。 解决方案：后端输出页面的时候将参数进行转义。如&lt;script&gt;转义成 &lt;script&gt;过滤用户输入。 注意：xss攻击的地方很多，html、css、js都有可能会被注入威胁。 三、CSRF跨站请求伪造攻击 攻击方式：利用收集到的用户cookie或其他敏感信息，使用httpclient等工具模拟请求，盗取用户信息。 解决方案：验证HTTP Referer字段，服务器端生成一次性token，请求时验证token。 四、DoS攻击 攻击方式：Ping Flood攻击即利用ping命令不停的发送的数据包到服务器，占用服务器带宽，直到网络瘫痪。SYN Flood攻击即利用tcp协议原理，伪造受害者的ip地址，一直保持与服务器的连接，导致受害者连接服务器的时候拒绝服务。 解决方案：设置防火墙。使用阿里云高防IP。 五、ARP欺骗——常见的窃取资料的安全性问题 攻击方式：利用ARP欺骗，伪造成网关，让受害者的数据经过攻击者的电脑，从而抓取别人的用户信息。 解决方案：强烈要求数据必须加密传输，启动https协议。 六、中间人攻击（会话劫持）—–常见的窃取资料的安全性问题 攻击方式：劫持会话cookies，把受害者（A）与受害者（B）之间通信经过攻击者的电脑。（常见于在线聊天系统） 解决方案：用户进行二次验证，随机产生会话ID，会话cookies设置httponly。 cookies的两种类型 会话cookies : 建立会话的cookies，关闭浏览器失效。 持久性cookies: 持久保存本地的cookies，到期失效。 会话cookies ！= session cookies会受到XSS，CSRF攻击。（把恶意代码存放到cookies里面） 七、其他注意的点 限制上传文件的类型和权限，防止用户上传jsp和服务器脚本！ 删除服务器默认的示例demo，如tomcat下的webapps下的默认文件！ 关注使用到的技术的更新，如：struts2的远程执行漏洞。及时更新到bug fixed版本。 转自如梦技术]]></content>
      <tags>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat重新部署和备份脚本]]></title>
    <url>%2F2017%2F06%2F06%2Ftomcat-e9-87-8d-e5-90-af-e8-84-9a-e6-9c-ac%2F</url>
    <content type="text"><![CDATA[查询指定服务的tomcat，如果存在对应pid则进行重启，同时对历史版本war包备份 123456789101112131415161718192021222324252627282930313233343536373839404142#!/bin/bash#defined TOMCAT_HOME=&quot;/data/server/tomcat-adminstock&quot;TOMCAT_PORT=20007PROJECT=adminstockTomcatID=$(ps -ef |grep tomcat |grep -w $TOMCAT_HOME|grep -v &apos;grep&apos;|awk &apos;&#123;print $2&#125;&apos;)if [ -n $TomcatID ]; then sudo &quot;$TOMCAT_HOME&quot;/bin/shutdown.sh -force echo &quot;=============tomcat shutdown=================&quot; else echo &quot;=============tomcat not start ===============&quot;fi#check tomcat processtomcat_pid=`/usr/sbin/lsof -n -P -t -i :$TOMCAT_PORT`echo &quot;current :&quot; $tomcat_pidwhile [ -n &quot;$tomcat_pid&quot; ]do sleep 5 tomcat_pid=`/usr/sbin/lsof -n -P -t -i :$TOMCAT_PORT` echo &quot;scan tomcat adminstock pid :&quot; $tomcat_piddone#publish projectecho &quot;scan no tomcat adminstock pid,$PROJECT publishing&quot;sudo rm -rf &quot;$TOMCAT_HOME&quot;/webapps/$PROJECT*sudo cp /data/server/jenkins_slave/workspace/MN_ADMIN/adminstock/target/$PROJECT.war &quot;$TOMCAT_HOME&quot;/webapps/$PROJECT.war#bak projectBAK_DIR=/data/bak/$PROJECT/`date +%Y%m%d`sudo mkdir -p &quot;$BAK_DIR&quot;sudo cp &quot;$TOMCAT_HOME&quot;/webapps/$PROJECT.war &quot;$BAK_DIR&quot;/&quot;$PROJECT&quot;_`date +%H%M%S`.war#remove tmpsudo rm -rf /data/bak/$PROJECT*.war#start tomcatcd $TOMCAT_HOME/bin/sudo ./startup.shecho &quot;tomcat adminstock is starting,please try to access $PROJECT conslone url&quot; sleep 15echo &quot;-----------------------------adminstock-----------------------------&quot;]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用aliases记录(持续更新)]]></title>
    <url>%2F2017%2F06%2F02%2F-e5-b8-b8-e7-94-a8aliases-e8-ae-b0-e5-bd-95-e6-8c-81-e7-bb-ad-e6-9b-b4-e6-96-b0%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137# 文件按大小排序，lbys = ls by size alias lbys=&apos;ls -alhS&apos; # 文件按时间排序，lbyt = ls by time alias lbyt=&apos;ls -alht&apos;# 重新运行上一条命令，并将输出复制到剪贴板，cl = copy last alias cl=&apos;bash -c &quot;$(fc -ln -1)&quot; | pbcopy&apos; # 复制上一条命令 alias last=&apos;fc -ln -1 | pbcopy&apos; # 将当前剪贴板里的内容保存到某个文件里 alias new=&apos;pbpaste | cat &gt;&apos; alias save=&apos;pbpaste | cat &gt;&apos; alias myip=&apos;curl ifconfig.co&apos; # 代理 alias setproxy=&apos;export ALL_PROXY=socks5://127.0.0.1:1086&apos; alias unsetproxy=&apos;unset ALL_PROXY&apos; # life query alias tq=&quot;curl wttr.in/shenzhen&quot; # brew alias bs=&quot;brew services&quot; alias bsl=&quot;brew services list&quot; # editor alias vi=&quot;vim&quot; alias edit=&quot;vim&quot; alias grep=&quot;grep -E --color&quot; alias egrep=&quot;egrep --color=auto&quot; alias fgrep=&quot;fgrep --color=auto&quot; alias mkdir=&quot;mkdir -pv&quot; alias tf=&apos;tail -f&apos; #动态查看文件变化 alias af=&quot;awk -F &apos;\t&apos; &apos;&#123;print NF&#125;&apos;&quot; #查看文件列数，用\t分隔，最常用，其实也可以搞个通用的，接收参数 alias wl=&apos;wc -l&apos; #统计行数 alias c=&quot;clear&quot;#清屏 alias cls=&quot;clear&quot; #清屏 alias cp=&apos;cp -v&apos; alias cpr=&apos;cp -r&apos; alias mv=&apos;mv -v&apos; alias df=&quot;df -h&quot; alias du=&quot;du -h&quot; alias dus=&quot;du -s&quot;#磁盘 alias du0=&quot;du --max-depth=0&quot; alias du1=&quot;du --max-depth=1&quot; alias last=&quot;last -a&quot; alias free=&apos;free -m&apos; #Notice: install colordiff alias diff=&apos;colordiff&apos; alias vd=&apos;vimdiff&apos; #vim diff两个文件 alias tree=&apos;tree -C&apos; alias dfind=&apos;find -type d -name&apos;#查找文件夹 alias ffind=&apos;find -type f -name&apos; #查找文件 alias chux=&apos;chmod u+x&apos; #该权限 # tool alias rmpyc=&apos;find . -name &quot;*.pyc&quot; -exec rm -rf &#123;&#125; \; &gt;&gt; /dev/null 2&gt;&amp;1&apos; #递归删除目录下所有pyc alias rmlog=&apos;rm *.log;rm *.log.*&apos; alias now=&apos;date +&quot;%Y-%m-%d %T&quot;&apos; # for svn alias rmsvn=&apos;find . -name &quot;.svn&quot; -exec rm -rf &#123;&#125; \; &gt;&gt; /dev/null 2&gt;&amp;1&apos;#递归删除目录下所有.svn alias svnci=&apos;svn ci -m &quot;commit by $USER&quot; &apos; #便捷操作，适用一些不重要文件的，不建议适用哈 alias svnst=&apos;svn st&apos; #少一个空格，少一个是一个......囧 # for go alias gor=&apos;go run&apos; alias gob=&apos;go build&apos; #for tmux alias tm=&apos;tmux -2&apos; alias tmux=&apos;tmux -2&apos; alias tma=&apos;tmux -2 attach&apos; alias tmx=&apos;tmuxinator&apos; # chdir alias ..=&quot;cd ..&quot; alias cdd=&quot;cd ..&quot; alias cd..=&quot;cd ..&quot; alias ...=&quot;cd ../..&quot; alias ....=&quot;cd ../../..&quot; alias .....=&quot;cd ../../../..&quot; alias .4=&apos;cd ../../../../&apos; alias .5=&apos;cd ../../../../..&apos; alias -- -=&apos;cd -&apos; alias cds=&apos;echo &quot;`pwd`&quot; &gt; ~/.cdsave&apos; #cd save : save where i am alias cdb=&apos;cd &quot;`cat ~/.cdsave`&quot;&apos; # cd back # processes alias pg=&apos;ps -ef | grep&apos; # services alias ms=&apos;mysql -uroot --password=&quot;&quot;&apos; #mysql alias ssh=&apos;ssh -2&apos;# ls for mac alias ll=&apos;ls -al&apos; #ls相关，这里--color需要根据终端设 alias lx=&apos;ls -lhBX&apos; #sort by extension alias lz=&apos;ls -lhrS&apos; #sort by size alias lt=&apos;ls -lhrt&apos; #sort by date 最常用到，ls -rt，按修改时间查看目录下文件 alias lsd=&apos;find . -maxdepth 1 -type d | sort&apos; #列出所有目录 alias sl=&apos;ls&apos; # net alias pong=&apos;ping -c 5 &apos; #ping，限制 alias ports=&apos;netstat -tulanp&apos; alias myip=&apos;curl ifconfig.me&apos; # useful functions #根据文件类型解压 #extract()&#123; ext()&#123; if [ -f $1 ]; then case $1 in *.tar.bz2) tar xjf $1 ;; *.tar.gz) tar xzf $1 ;; *.bz2) bunzip2 $1 ;; *.rar) unrar e $1 ;; *.gz) gunzip $1 ;; *.tar) tar xf $1 ;; *.tbz2) tar xjvf $1 ;; *.tgz) tar xzvf $1 ;; *.zip) unzip $1 ;; *.Z) uncompress $1 ;; *.7z) 7z x $1 ;; *) echo &quot;&apos;$1&apos; cannot be extracted via extract()&quot; ;; esac else echo &quot;&apos;$1&apos; is not a valid file&quot; fi &#125; #压缩 mktar()&#123; tar cvf &quot;$&#123;1%%/&#125;.tar&quot; &quot;$&#123;1%%/&#125;/&quot;; &#125; mktgz()&#123; tar cvzf &quot;$&#123;1%%/&#125;.tar.gz&quot; &quot;$&#123;1%%/&#125;/&quot;; &#125; mktbz()&#123; tar cvjf &quot;$&#123;1%%/&#125;.tar.bz2&quot; &quot;$&#123;1%%/&#125;/&quot;; &#125; #分屏同时打开多个文件 #vim -oN filea fileb filec vo()&#123; vim -o$# $* &#125; #创建一个目录并跳转到 #make a dir and cd into it mcd()&#123; mkdir -pv &quot;$@&quot; cd &quot;$@&quot; &#125; #保存一个文件到tmp # save a file to ~/tmp saveit() &#123; cp $1 $&#123;HOME&#125;/tmp/$&#123;1&#125;.saved &#125; #交换两个文件 # switch two files (comes in handy) switchfile() &#123; mv $1 $&#123;1&#125;.tmp &amp;&amp; mv $2 $1 &amp;&amp; mv $&#123;1&#125;.tmp $2 &#125; #查看自己常用命令top n # View most commonly used commands. depends on your history output format used()&#123; if [ $1 ] then history | awk &apos;&#123;print $4&#125;&apos; | sort | uniq -c | sort -nr | head -n $1 else history | awk &apos;&#123;print $4&#125;&apos; | sort | uniq -c | sort -nr | head -n 10 fi &#125; #最常用，复制一个路径过来时，不用修改，可以到达路径的最深一层目录 #if dir,cd into it. if file ,cd into where the file is goto()&#123; [ -d &quot;$1&quot; ] &amp;&amp; cd &quot;$1&quot; || cd &quot;$(dirname &quot;$1&quot;)&quot;; &#125; #临时文件 mvtmp()&#123; mv $1 ~/tmp/ &#125; function cptmp()&#123; cp -r $1 ~/tmp/ &#125; # Serve directory on localhost:80 servedir()&#123; sudo python -m SimpleHTTPServer 80 &#125; # applications # aira2 alias download=&apos;aria2c&apos; alias dl=&apos;aria2c&apos; # thefuck # tmp alias jj=&quot;~/workspace/bin/gossh&quot; alias spi=&quot;sudo /usr/local/bin/pip install&quot; alias jjj=&quot;~/bin/kw3gossh&quot; # install https://github.com/what-studio/profiling alias pypro=&apos;python -m profiling profile&apos; # install promptpython alias pt=&apos;ptpython&apos;]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>效率工具</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka集群搭建及简介]]></title>
    <url>%2F2017%2F06%2F02%2Fkafka-e9-9b-86-e7-be-a4-e6-90-ad-e5-bb-ba-e5-8f-8a-e7-ae-80-e4-bb-8b%2F</url>
    <content type="text"><![CDATA[Kafka集群搭建需要集群的服务器上各自下载安装程序wget https://www.apache.org/dyn/closer.cgi?path=/kafka/0.10.2.1/kafka_2.12-0.10.2.1.tgz tar -xzvf kafka_2.12-0.10.2.1.tgz 分别修改server.propertiesbroker.id=0（1、2、3...） zookeeper.connect=192.168.199.248:2181,192.168.199.170:2181,192.168.199.146:2181 使用守护进程方式运行bin/zookeeper-server-start.sh -daemon config/zookeeper.properties Kafka的特性 高吞吐量、低延迟：kafka每秒可以处理几十万条消息，它的延迟最低只有几毫秒 可扩展性：kafka集群支持热扩展 持久性、可靠性：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失 容错性：允许集群中节点失败（若副本数量为n,则允许n-1个节点失败） 高并发：支持数千个客户端同时读写 Kafka架构Broker Kafka集群包含一个或多个服务器，这种服务器被称为broker ### Topic 每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处） Partition Parition是物理上的概念，每个Topic包含一个或多个Partition.每个partition物理上对应一个文件夹，该文件夹存储该partition的所有消息和索引文件。 正因为消息是以文件的形式存储的，所以无论消息是否被消费，kafka都会保留所有消息，有两种策略可以删除旧数据： &gt; 基于时间：log.retention.hours=168 基于大小：log.retention.bytes=1073741824 日志 如果一个topic的名称为”my_topic”,有3个partitions，那么每个partitions 都分别对应有一个目录 my_topic_0、my_topic_1、my_topic_2；每个目录里放着具体的数据文件：消息跟索引文件。 ### Producer 负责发布消息到Kafka broker 一般情况下存在三种情况： 1. At most once 消息可能会丢，但绝不会重复传输； At least one 消息绝不会丢，但可能会重复传输； Exactly once 每条消息肯定会被传输一次且仅传输一次； 当 producer 向 broker 发送消息时，一这条消息被 commit，由于 replication 的存在，它就不会丢。但是如果 producer 发送数据给 broker 后，遇到网络问题而造成通信中断，那 Producer 就无法判断该条消息是否已经 commit。虽然 Kafka 无法确定网络故障期间发生了什么，但是 producer 可以生成一种类似于主键的东西，发生故障时幂等性的重试多次，这样就做到了 Exactly once，但目前还并未实现。所以目前默认情况下一条消息从 producer 到 broker 是确保了 At least once，可通过设置 producer 异步发送实现At most once。 路由机制 producer 发送消息到 broker 时，会根据分区算法选择将其存储到哪一个 partition。其路由机制为： 1. 指定了 patition，则直接使用； 未指定 patition 但指定 key，通过对 key 的 value 进行hash 选出一个 patition；默认情况下，Kafka根据传递消息的key来进行分区的分配，即hash(key)%numPartitions，这就保证了相同key的消息一定会被路由到相同的分区 patition 和 key 都未指定，使用轮询选出一个 patition。 Kafka几乎就是随机找一个分区发送无key的消息，然后把这个分区号加入到缓存中以备后面直接使用——当然了，Kafka本身也会清空该缓存（默认每10分钟或每次请求topic元数据时） 如果你的分区数是N，那么最好线程数也保持为N，这样通常能够达到最大的吞吐量。 ### Consumer 消息消费者，向Kafka broker读取消息的客户端。 ### Consumer Group kafka中消费者组有两个概念：队列：消费者组（consumer group）允许同名的消费者组成员瓜分处理。发布-订阅：允许你广播消息给多个消费者组（不同名）。每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。 ### 分布式 每个服务器处理它分到的分区。 根据配置每个分区还可以复制到其它服务器作为备份容错。 每个分区有一个leader，零或多个follower。Leader处理此分区的所有的读写请求，而follower被动的复制数据。如果leader宕机，其它的一个follower会被推举为新的leader。 一台服务器可能同时是一个分区的leader，另一个分区的follower。 这样可以平衡负载，避免所有的请求都只让一台或者某几台服务器处理。 Kafka拓扑结构 1一个典型的kafka集群中包含若干producer（可以是web前端产生的page view，或者是服务器日志，系统CPU、memory等），若干broker（Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高），若干consumer group，以及一个Zookeeper集群。Kafka通过Zookeeper管理集群配置，选举leader，以及在consumer group发生变化时进行rebalance。producer使用push模式将消息发布到broker，consumer使用pull模式从broker订阅并消费消息。 消费示例 通常来讲，消息模型可以分为两种， 队列和发布-订阅式。 队列的处理方式是 一组消费者从服务器读取消息，一条消息只有其中的一个消费者来处理。在发布-订阅模型中，消息被广播给所有的消费者，接收到消息的消费者都可以处理此消息。Kafka为这两种模型提供了单一的消费者抽象模型： 消费者组 （consumer group）。 消费者用一个消费者组名标记自己。 一个发布在Topic上消息被分发给此消费者组中的一个消费者。 假如所有的消费者都在一个组中，那么这就变成了queue模型。 假如所有的消费者都在不同的组中，那么就完全变成了发布-订阅模型。 更通用的， 我们可以创建一些消费者组作为逻辑上的订阅者。每个组包含数目不等的消费者， 一个组内多个消费者可以用来扩展性能和容错。正如上图所示： 12个kafka集群托管4个分区（P0-P3），2个消费者组，消费组A有2个消费者实例，消费组B有4个。 kafka broker宕机 &amp; leader选举机制为了保证高可用，每个分区都会有一定数量的副本（replica）。这样如果有部分服务器不可用，副本所在的服务器就会接替上来，保证应用的持续性。但是，为了保证较高的处理效率，消息的读写都是在固定的一个副本上完成。这个副本就是所谓的Leader，而其他副本则是Follower。而Follower则会定期地到Leader上同步数据。 如果某个分区所在服务器宕机了，那么kafka会从该分区的其他副本中选择一个作为新的leader，之后所有的读写就转移到这个新的leader上来。 因此，leader的server承载了全部的请求压力,因此从集群的整体考虑,有多少个partitions就意味着有多少个”leader”,kafka会将”leader”均衡的分散在每个实例上,来确保整体的性能稳定. Leader选举 Kafka会在Zookeeper上针对每个Topic维护一个称为ISR（in-sync replica，已同步的副本）的集合，该集合中是一些分区的副本。只有当这些副本都跟Leader中的副本同步了之后，kafka才会认为消息已提交，并反馈给消息的生产者。如果这个集合有增减，kafka会更新zookeeper上的记录。 如果某个分区的Leader不可用，Kafka就会从ISR集合中选择一个副本作为新的Leader。如果只允许一台机器失败，那么需要两个副本，只允许两台机器失败，则需要三个副本。 这里有一种极端的情况：所有的ISR副本都失败了，此时有两种方法可选，一种是等待ISR集合中的副本复活，一种是选择任何一个立即可用的副本，而这个副本不一定是在ISR集合中。&gt; 第一种：如果要等待ISR副本复活，虽然可以保证一致性，但可能需要很长时间。第二种如果选择立即可用的副本，则很可能该副本并不一致。 1.每个线程的一个消费者一个简单的选择是给每个线程自己的消费者实例。 以下是这种方法的优点和缺点： PRO：最简单的实现PRO：它通常是最快的，因为不需要线程间协调PRO：它使每个分区的按顺序处理非常易于实现（每个线程都按照接收它们的顺序处理消息）。 CON：更多的消费者意味着更多的TCP连接到集群（每个线程一个）。一般来说，卡夫卡非常有效地处理连接，所以这通常是一个很小的成本。 CON：多个消费者意味着更多的请求被发送到服务器，稍微少一点的数据批处理可能会导致I / O吞吐量的下降。 CON：所有进程的总线程数将受到分区总数的限制。 2.消除和处理解耦另一种方法是让一个或多个消费者线程执行所有数据消耗，并将ConsumerRecords实例移交给实际处理记录处理的处理器线程池消耗的阻塞队列。 此选项同样具有优点和缺点： PRO：该选项允许独立扩展消费者和处理器的数量。这使得可以让单个消费者提供许多处理器线程，避免对分区的任何限制。 CON：跨处理器的保证顺序需要特别小心，因为线程将独立执行，因为稍后的数据块可能会由于线程执行时间的运行而在之前的数据块中被处理。对于没有订购要求的处理，这不是问题。 CON：手动提交的位置变得更加困难，因为它要求所有线程协调，以确保该分区的处理完成。这种方法有很多可能的变化。例如，每个处理器线程可以具有自己的队列，并且消费者线程可以使用TopicPartition散列到这些队列中，以确保按顺序消费并简化提交。]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>消息队列</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper集群搭建(三台)]]></title>
    <url>%2F2017%2F06%2F02%2Fzookeeper-e9-9b-86-e7-be-a4-e6-90-ad-e5-bb-ba-e4-b8-89-e5-8f-b0%2F</url>
    <content type="text"><![CDATA[zookeeper服务集群搭建(基于docker) 如果启动容器的时候使用–net host模式，那么这个容器将和宿主机共用一个Network Namespace。这时Docker Engine将不会为容器创建veth pair并配置IP等，而是直接使用宿主机的网络配置，就如直接跑在宿主机中的进程一样。注意，这时容器的其他资源，如文件系统、进程列表等还是和宿主机隔离的。 利用host网络，容器的IP地址和机器节点一致，这样我们在部署容器之前就能确定每个ZK节点的IP地址。我们可以分别在三台的机器上用host模式启动一个ZK容器并配置一个分布式集群。 对三台服务器进行zookeeper集群搭建：服务器A：docker run -d \ --name=zk1 \ --net=host \ -e SERVER_ID=1 \ -e ADDITIONAL_ZOOKEEPER_1=server.1=192.168.199.248:2888:3888 \ -e ADDITIONAL_ZOOKEEPER_2=server.2=192.168.199.170:2888:3888 \ -e ADDITIONAL_ZOOKEEPER_3=server.3=192.168.199.146:2888:3888 \ registry.aliyuncs.com/acs-sample/zookeeper:3.4.8 服务器B：docker run -d \ --name=zk2 \ --net=host \ -e SERVER_ID=2 \ -e ADDITIONAL_ZOOKEEPER_1=server.1=192.168.199.248:2888:3888 \ -e ADDITIONAL_ZOOKEEPER_2=server.2=192.168.199.170:2888:3888 \ -e ADDITIONAL_ZOOKEEPER_3=server.3=192.168.199.146:2888:3888 \registry.aliyuncs.com/acs-sample/zookeeper:3.4.8 服务器C：docker run -d \ --name=zk3 \ --net=host \ -e SERVER_ID=3 \ -e ADDITIONAL_ZOOKEEPER_1=server.1=192.168.199.248:2888:3888 \ -e ADDITIONAL_ZOOKEEPER_2=server.2=192.168.199.170:2888:3888 \ -e ADDITIONAL_ZOOKEEPER_3=server.3=192.168.199.146:2888:3888 \registry.aliyuncs.com/acs-sample/zookeeper:3.4.8 验证集群搭建情况telnet 192.168.199.248 2181 &gt; stat 可以看到各台服务器状态，一台leader和多台flower即可说明集群搭建成功]]></content>
      <tags>
        <tag>Docker</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux定时删除日志文件执行脚本]]></title>
    <url>%2F2017%2F05%2F22%2Flinux-e5-ae-9a-e6-97-b6-e5-88-a0-e9-99-a4-e6-97-a5-e5-bf-97-e6-96-87-e4-bb-b6-e6-89-a7-e8-a1-8c-e8-84-9a-e6-9c-ac%2F</url>
    <content type="text"><![CDATA[linux是一个很能自动产生文件的系统，日志、邮件、备份等。虽然现在硬盘廉价，我们可以有很多硬盘空间供这些文件浪费，让系统定时清理一些不需要的文件很有一种爽快的事情。不用你去每天惦记着是否需要清理日志，不用每天收到硬盘空间不足的报警短信，想好好休息的话，让我们把这个事情交给机器定时去执行吧。 1.删除文件命令 find 对应目录 -mtime +天数 -name &quot;文件名&quot; -exec rm -rf {} \; 实例命令： find /opt/soft/log/ -mtime +30 -name &quot;*.log&quot; -exec rm -rf {} \; 说明： 将/opt/soft/log/目录下所有30天前带”.log”的文件删除。 具体参数说明如下： find：linux的查找命令，用户查找指定条件的文件； /opt/soft/log/：想要进行清理的任意目录； -mtime：标准语句写法； +30：查找30天前的文件，这里用数字代表天数； &quot;*.log&quot;：希望查找的数据类型，”.jpg”表示查找扩展名为jpg的所有文件，”“表示查找所有文件，这个可以灵活运用，举一反三； -exec：固定写法； rm -rf：强制删除文件，包括目录； {} \;：固定写法，一对大括号+空格++; 2.计划任务 若嫌每次手动执行语句太麻烦，可以将这小语句写到一个可执行shell脚本文件中，再设置cron调度执行，那就可以让系统自动去清理相关文件。 2.1创建shell： touch /opt/soft/bin/auto-del-30-days-ago-log.sh chmod +x auto-del-30-days-ago-log.sh 新建一个可执行文件auto-del-30-days-ago-log.sh，并分配可运行权限 2.2编辑shell脚本： vi auto-del-30-days-ago-log.sh 编辑auto-del-30-days-ago-log.sh文件如下： #!/bin/sh find /opt/soft/log/ -mtime +30 -name “*.log” -exec rm -rf {} \; ok，保存退出(:wq)。 2.3计划任务： #crontab -e 将auto-del-30-days-ago-log.sh执行脚本加入到系统计划任务，到点自动执行 输入： 10 0 * /opt/soft/log/auto-del-7-days-ago-log.sh &gt;/dev/null 2&gt;&amp;1 这里的设置是每天凌晨0点10分执行auto-del-7-days-ago-log.sh文件进行数据清理任务了。]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>日志收集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP连接的报文到达确认机制]]></title>
    <url>%2F2017%2F05%2F09%2Ftcp-e8-bf-9e-e6-8e-a5-e7-9a-84-e6-8a-a5-e6-96-87-e5-88-b0-e8-be-be-e7-a1-ae-e8-ae-a4-e6-9c-ba-e5-88-b6%2F</url>
    <content type="text"><![CDATA[利用tcpdump命令查看Netty建立TCP连接时，报文数据的流向，如图 ACK: 是对接收到的数据的最高序列号的确认，并向发送端返回一个下次接收时期望的TCP数据包的序列号（Ack Number）。例如， 主机A发送的当前数据序号是400，数据长度是100，则接收端收到后会返回一个确认号是500的确认号给主机A。 SEQ: TCP会话的每一端都包含一个32位（bit）的序列号，该序列号被用来跟踪该端发送的数据量。每一个包中都包含序列号，在接收端则通过确认号用来通知发送端数据成功接收。]]></content>
      <tags>
        <tag>网络协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 修改默认的镜像存储位置]]></title>
    <url>%2F2017%2F05%2F03%2Fdocker--e4-bf-ae-e6-94-b9-e9-bb-98-e8-ae-a4-e7-9a-84-e9-95-9c-e5-83-8f-e5-ad-98-e5-82-a8-e4-bd-8d-e7-bd-ae%2F</url>
    <content type="text"><![CDATA[备份&amp;文件同步 首先，备份 fstab 文件，文件位于 /etc/fstab sudo cp /etc/fstab /etc/fstab.$(date +%Y-%m-%d) 关闭 docker 服务，用 rsync 同步 /var/lib/docker到新位置.yum -y intall rsync service docker stop mkdir /data/docker rsync -aXS /var/lib/docker/. /data/docker/ 花费时间取决于/var/lib/docker的大小，建议在装docker后立即修改默认的镜像存储路径，等到了系统盘饱满就尴尬了，这里是有过惨痛经历。 docker的安装方式最好根据官方的文档安装docker社区版(CE)，能少填一些坑。 修改fstab &amp; 重新挂载 在该文件中把下面一行添加到 fstab 里，将新位置挂载到 /var/lib/docker /data/docker /var/lib/docker none bind 0 0 然后重新挂载mount –a 验证：df /var/lib/docker/]]></content>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitLab持续集成之gitlab-runner构建使用]]></title>
    <url>%2F2017%2F05%2F02%2Fgitlab-e6-8c-81-e7-bb-ad-e9-9b-86-e6-88-90-e4-b9-8bgitlab-runner-e9-95-9c-e5-83-8f-e6-9e-84-e5-bb-ba%2F</url>
    <content type="text"><![CDATA[流程图 安装gitlab-runner 官方推荐使用包管理工具安装的方式，推荐使用清华的镜像：Gitlab CI Multi Runner 对于需要安装历史版本的可以从这里找： 传送门 本文将更多去介绍采用docker构建镜像的方式去run一个安装了gitlab-runner的容器。 前期准备 由于自用Gitlab的版本比较低，需要找低版本的gitlab-runner，而官方对centOS7的支持不是太友好的感觉，折腾了两天没搞好centOS下的镜像。所以乌邦图将就用一下，达到目的就好。 附兼容表： Dockerfile(基于Ubuntu14.04的镜像 Docker JDK8 Gradle)FROM ubuntu:14.04 ADD dumb-init_1.0.2_amd64 /usr/bin/dumb-init RUN chmod +x /usr/bin/dumb-init RUN apt-get update -y &amp;&amp; \ apt-get upgrade -y &amp;&amp; \ apt-get install -y ca-certificates wget git apt-transport-https curl vim nano software-properties-common python-software-properties COPY gitlab-ci-multi-runner_amd64.deb /tmp/ RUN dpkg -i /tmp/gitlab-ci-multi-runner_amd64.deb; \ apt-get update &amp;&amp; \ apt-get -f install -y &amp;&amp; \ apt-get clean &amp;&amp; \ rm -rf /var/lib/apt/lists/* &amp;&amp; \ rm /tmp/gitlab-ci-multi-runner_amd64.deb &amp;&amp; \ gitlab-runner --version &amp;&amp; \ mkdir -p /etc/gitlab-runner/certs &amp;&amp; \ chmod -R 700 /etc/gitlab-runner # 安装Docker RUN curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - RUN add-apt-repository \ &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \ $(lsb_release -cs) \ stable&quot; RUN apt-get update -y RUN apt-get -y install docker-ce RUN apt-get clean &amp;&amp; \ rm -rf /var/lib/apt/lists/ # 安装jdk8 RUN apt-get update &amp;&amp; apt-get -y upgrade &amp;&amp; apt-get -y install software-properties-common &amp;&amp; add-apt-repository ppa:webupd8team/java -y &amp;&amp; apt-get update RUN (echo oracle-java8-installer shared/accepted-oracle-license-v1-1 select true | /usr/bin/debconf-set-selections) &amp;&amp; apt-get install -y oracle-java8-installer oracle-java8-set-default ENV JAVA_HOME /usr/lib/jvm/java-8-oracle ENV PATH $JAVA_HOME/bin:$PATH RUN apt-get clean RUN rm -rf /var/lib/apt/lists RUN rm -rf /var/cache/oracle-jdk${JAVA_VER}-installer # 安装Gradle，Gradle的源原件需要手工下载，速度更快 RUN curl -o /tmp/gradle.zip https://services.gradle.org/distributions/gradle-3.4.1-bin.zip ADD gradle-3.4.1 /usr/local/gradle ENV GRADLE_HOME /usr/local/gradle ENV PATH $PATH:$GRADLE_HOME/bin # 为gitlab-runner用户增加docker权限 RUN usermod -aG docker gitlab-runner COPY entrypoint / RUN chmod +x /entrypoint VOLUME [&quot;/etc/gitlab-runner&quot;, &quot;/home/gitlab-runner&quot;] ENTRYPOINT [&quot;/usr/bin/dumb-init&quot;, &quot;/entrypoint&quot;] CMD [&quot;run&quot;, &quot;--user=gitlab-runner&quot;, &quot;--working-directory=/home/gitlab-runner&quot;] 运行 Use Docker socket bindingdocker run --name gitlab-runner -d -v /data/gitlab-ci/config:/etc/gitlab-runner/ -v /var/run/docker.sock:/var/run/docker.sock c0cacb75b262 参考 https://docs.gitlab.com/ce/ci/docker/using_docker_build.html#use-docker-in-docker-executor https://docs.gitlab.com/runner/install/ https://hub.docker.com/r/sameersbn/gitlab/ https://docs.gitlab.com/runner/commands/README.html]]></content>
      <tags>
        <tag>持续集成</tag>
        <tag>Gitlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitLab持续集成之.gitlab-ci.yml配置实例]]></title>
    <url>%2F2017%2F05%2F02%2Fgitlab-e6-8c-81-e7-bb-ad-e9-9b-86-e6-88-90-e4-b9-8b-gitlab-ci-yml-e9-85-8d-e7-bd-ae-e5-ae-9e-e4-be-8b%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334variables: # 对应阿里云镜像仓库的namespace，全局唯一 GROUP: &quot;name&quot; PROJECT: &quot;project&quot; # 定义 stages pipeline顺序执行 stages: - build - test - deploy build: stage: build #只允许构建的分支 only: develop #执行shell命令 script: - cd belgrade - rm -rf build - /usr/local/gradle/bin/gradle build --info ## 构建镜像 - sudo docker build -t &quot;$GROUP/$PROJECT:develop&quot; . - sudo docker tag $GROUP/$PROJECT:develop registerUrl/$GROUP/$PROJECT:develop - echo &apos;build done!&apos; andTest: stage: test only: - develop script: - echo &apos;test done!&apos; andPush: stage: deploy only: - develop script: #推送镜像到docker私仓 - sudo docker push registerUrl/$GROUP/$PROJECT:develop - echo &apos;push done!&apos; #触发jenkins的deploy任务 - curl -s -u username:password http://JENKINS_SERVER/joba/deplody-belgrade/build?token=YOUR_TOCKEN - echo &apos;deplody done!&apos; 参考 https://docs.gitlab.com/ce/ci/yaml/]]></content>
      <tags>
        <tag>持续集成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jenkins的远程trigger服务]]></title>
    <url>%2F2017%2F05%2F02%2Fjenkins-e7-9a-84-e8-bf-9c-e7-a8-8btrigger-e6-9c-8d-e5-8a-a1%2F</url>
    <content type="text"><![CDATA[配置 在需要被执行的job的构建触发器中选择触发远程构建 (例如,使用脚本)： ###调用 curl -s -u username:password http://CI_SERVER/jobs/jobname/build?token=TOKEN]]></content>
      <tags>
        <tag>Jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何恢复在Git节点游离状态（detached HEAD）后的commit记录]]></title>
    <url>%2F2017%2F03%2F23%2F-e5-a6-82-e4-bd-95-e6-81-a2-e5-a4-8d-e5-9c-a8git-e8-8a-82-e7-82-b9-e6-b8-b8-e7-a6-bb-e7-8a-b6-e6-80-81-ef-bc-88detached-head-ef-bc-89-e5-90-8e-e7-9a-8%2F</url>
    <content type="text"><![CDATA[游离状态（detached HEAD）：不指向任何分支的HEAD节点，并且可以在上面进行commit操作 记录今天遇到的一个状况以及事后的解决方案 在detached HEAD状态下进行了commit操作，此时无法push到任何仓库，由于没太注意当前的分支状态就checkout到其他分支，之前的修改一并消失了。 为了找回“宝贵的记录”，查阅资料，寻得了恢复记录的办法如下： 利用git reflog命令，可以查看所有分支的所有操作记录（包括commit和reset的操作），查找最近十次commit记录： git reflog show HEAD@{now} -10 将游离状态的节点指向新建分支： git branch newBranch 37fdae9421 合并这个新分支到你想要的地方，ok记录回来了。Thanks God。]]></content>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gradle构建区分环境的编译打包]]></title>
    <url>%2F2017%2F03%2F18%2Fgradle-e6-9e-84-e5-bb-ba-e5-8c-ba-e5-88-86-e7-8e-af-e5-a2-83-e7-9a-84-e7-bc-96-e8-af-91-e6-89-93-e5-8c-85%2F</url>
    <content type="text"><![CDATA[根据WAR包的用途，有三个命令： 一、发布生产的版本：adminstock-{version}.{build_version}-RELEASE.wargradle clean release 二、本地调试版本：adminstock-{version}.war （不带编译次数，便于IDEA指定路径） gradle clean war 三、快照版本：adminstock-{version}.{build_version}-SNAPSHOT.wargradle clean snapshop 默认情况下: 只有发布Release版本会读取resources-prod 下的配置, 本地测试和快照版本读取resources-dev的配置， resources目录放各个环境的通用配置 &gt; gradle.properties 维护版本信息 1234version：1.0.2 //当前版本号build_version: 1//当前构建号只有发布生产版本时，构建号才会累加 具体实施build.gradle12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394apply plugin: &apos;java&apos;apply plugin: &apos;maven&apos; apply plugin: &apos;war&apos; apply plugin: &apos;idea&apos; group = &apos;com.iwannarun.demo&apos; compileJava.options.encoding = &apos;UTF-8&apos; sourceCompatibility = 1.8 // 设置默认 resources 为开发环境状态 // 这儿如果不设置的话，idea 编译时只会默认获取 resources 目录（dao映射的xml也需要加入） sourceSets &#123; main &#123; resources &#123; srcDirs = [&apos;src/main/java&apos;,&apos;src/main/resources&apos;, &apos;src/main/resources-dev&apos;] &#125; &#125; &#125; repositories &#123; //依赖查找顺序：本地-&gt;私仓-&gt;阿里云-&gt;官方 mavenLocal() maven &#123; url &quot;http://yourrepository/nexus/repository/&quot; &#125; maven &#123;url &quot;http://maven.aliyun.com/nexus/content/groups/public&quot; &#125; mavenCentral()&#125; dependencies &#123; //以下是项目依赖的第三方库，简化了maven中的dependencie compile group: &apos;org.apache.httpcomponents&apos;, name: &apos;httpcore&apos;, version:&apos;4.4&apos; compile group: &apos;com.github.pagehelper&apos;, name: &apos;pagehelper&apos;, version:&apos;3.6.3&apos; compile group: &apos;dom4j&apos;, name: &apos;dom4j&apos;, version:&apos;1.6.1&apos; compile group: &apos;jaxen&apos;, name: &apos;jaxen&apos;, version:&apos;1.1.6&apos; compile group: &apos;org.bouncycastle&apos;, name: &apos;bcprov-jdk15on&apos;, version:&apos;1.47&apos; compile group: &apos;jstl&apos;, name: &apos;jstl&apos;, version:&apos;1.2&apos; compile group: &apos;taglibs&apos;, name: &apos;standard&apos;, version:&apos;1.1.2&apos; compile group: &apos;org.jibx&apos;, name: &apos;jibx-run&apos;, version:&apos;1.2&apos; compile group: &apos;org.jdom&apos;, name: &apos;jdom&apos;, version:&apos;1.1.3&apos; compile group: &apos;jsch&apos;, name: &apos;jsch&apos;, version:&apos;0.1.29&apos; compile group: &apos;xpp3&apos;, name: &apos;xpp3&apos;, version:&apos;1.1.3.4.O&apos; compile group: &apos;commons-lang&apos;, name: &apos;commons-lang&apos;, version:&apos;2.4&apos; compile group: &apos;commons-beanutils&apos;, name: &apos;commons-beanutils&apos;, version:&apos;1.8.3&apos; compile group: &apos;com.fasterxml.jackson.core&apos;, name: &apos;jackson-core&apos;, version:&apos;2.1.0&apos; compile group: &apos;com.fasterxml.jackson.core&apos;, name: &apos;jackson-databind&apos;, version:&apos;2.1.0&apos; compile group: &apos;com.fasterxml.jackson.core&apos;, name: &apos;jackson-annotations&apos;, version:&apos;2.1.0&apos; compile group: &apos;commons-fileupload&apos;, name: &apos;commons-fileupload&apos;, version:&apos;1.3&apos; compile group: &apos;commons-httpclient&apos;, name: &apos;commons-httpclient&apos;, version:&apos;3.0.1&apos; compile group: &apos;com.baidu.ueditor&apos;, name: &apos;ueditor&apos;, version:&apos;1.1.0&apos; compile(group: &apos;org.springframework.data&apos;, name: &apos;spring-data-mongodb&apos;, version:&apos;1.9.2.RELEASE&apos;) &#123; //排除冲突的依赖 exclude(module: &apos;spring-core&apos;) exclude(module: &apos;spring-tx&apos;) exclude(module: &apos;spring-context&apos;) exclude(module: &apos;spring-beans&apos;) exclude(module: &apos;spring-expression&apos;) &#125; compile group: &apos;com.aliyun&apos;, name: &apos;oss&apos;, version:&apos;2.2.3&apos; &#125; ext &#123; /**用于记录编译的次数 * 项目中调用，用于生成编译数字，此数字可以作为版本号的一个组成部分。 * * 存储位置为子项目下的 build_number.properties,此文件与项目的 build.gradle 平级 */ buildNumberIncrease = &#123; def versionPropsFile = new File(&apos;gradle.properties&apos;) if (versionPropsFile.canRead()) &#123; Properties versionProps = new Properties() versionProps.load(new FileInputStream(versionPropsFile)) def buildNumber = versionProps[&apos;build_version&apos;].toInteger() println &apos;-- Old build number: &apos; + buildNumber buildNumber++ println &apos;-- New build number: &apos; + buildNumber def runTasks = gradle.startParameter.taskNames //如果是发布模式，才对build进行自增 if (&apos;release&apos; in runTasks) &#123; versionProps[&apos;build_version&apos;] = (buildNumber).toString() versionProps.store(versionPropsFile.newWriter(), null) &#125; return buildNumber &#125; else &#123; throw new GradleException(&quot;Could not read version.properties!&quot;) &#125; &#125; &#125; // 设置 快照版本snapshot war 版本号 // 格式为：adminstock-&#123;version&#125;.&#123;x&#125;-SNAPSHOT.war task setSnapshotWarVersion &lt;&lt; &#123; println &apos;== Snapshot begin....&apos; def buildNumber = buildNumberIncrease() version += &apos;.&apos; + buildNumber version += &apos;-SNAPSHOT&apos; println &apos;-- Snapshot version: &apos; + version&#125; // 设置 release 包的版本号 // 格式为: adminstock-&#123;version&#125;.&#123;build_version&#125;-RELEASE.war // version 为大的版本号 build_version为当前构建号（只在release发布模式下会自增） task setReleaseWarVersion &lt;&lt; &#123; println &apos;== Release Begin...&apos; def buildNumber = buildNumberIncrease() version += &apos;.&apos; + buildNumber version += &apos;-RELEASE&apos; // 设置资源文件夹 sourceSets &#123; main &#123; resources &#123; srcDirs = [&quot;src/main/resources&quot;, &quot;src/main/resources-prod&quot;] &#125; &#125; &#125; println &apos;-- Release version: &apos; + version &#125; //发布war包共有一下三个命令： //命令一： clean war ===&gt; 默认读取resource-dev下的配置 //命令二：clean release // 编译发布用的 war 包 ===&gt;读取resource-prod下的配置 task release(dependsOn: [&apos;setReleaseWarVersion&apos;, &apos;war&apos;]) &lt;&lt; &#123; println &apos;== Release WAR Generate Over&apos; &#125; //命令三：clean snapshot // 编译快照版的 war 包 ===&gt; 读取resource-dev下的配置 task snapshot(dependsOn: [&apos;setSnapshotWarVersion&apos;, &apos;war&apos;]) &lt;&lt; &#123; println &apos;== Snapshot WAR Generate Over&apos; &#125;]]></content>
      <tags>
        <tag>Gradle</tag>
        <tag>构建工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven工程自动转换Gradle构建的方式]]></title>
    <url>%2F2017%2F03%2F18%2Fmaven-e5-b7-a5-e7-a8-8b-e8-87-aa-e5-8a-a8-e8-bd-ac-e6-8d-a2gradle-e6-9e-84-e5-bb-ba-e7-9a-84-e6-96-b9-e5-bc-8f%2F</url>
    <content type="text"><![CDATA[最近有个Maven工程转Gradle构建的需求，记录一下： Gradle安装Mac下可以选择brew或SDKMAN安装（官方推荐） brew install gradle sdk install gradle 3.4.1&apos; Maven转Gradle1gradle init --type pom]]></content>
      <tags>
        <tag>效率工具</tag>
        <tag>Gradle</tag>
        <tag>构建工具</tag>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitHub通过SS代理加速]]></title>
    <url>%2F2017%2F03%2F17%2Fgithub-e9-80-9a-e8-bf-87ss-e4-bb-a3-e7-90-86-e5-8a-a0-e9-80-9f%2F</url>
    <content type="text"><![CDATA[###设置代理仅对github.com有效 git config --global http.https://github.com.proxy socks5://127.0.0.1:1080 取消代理git config --global --unset http.https://github.com.proxy 全局代理12git config --global http.proxy &apos;socks5://127.0.0.1:1080&apos;git config --global https.proxy &apos;socks5://127.0.0.1:1080&apos;]]></content>
      <tags>
        <tag>Git</tag>
        <tag>效率工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Firewalld的基本使用]]></title>
    <url>%2F2017%2F03%2F08%2Ffirewalld-e7-9a-84-e5-9f-ba-e6-9c-ac-e4-bd-bf-e7-94-a8%2F</url>
    <content type="text"><![CDATA[启动： systemctl start firewalld 查看状态： systemctl status firewalld 停止： systemctl disable firewalld 禁用： systemctl stop firewalld systemctl是CentOS7的服务管理工具中主要的工具，它融合之前service和chkconfig的功能于一体。 启动一个服务：systemctl start firewalld.service 关闭一个服务：systemctl stop firewalld.service 重启一个服务：systemctl restart firewalld.service 显示一个服务的状态：systemctl status firewalld.service 在开机时启用一个服务：systemctl enable firewalld.service 在开机时禁用一个服务：systemctl disable firewalld.service 查看服务是否开机启动：systemctl is-enabled firewalld.service 查看已启动的服务列表：systemctl list-unit-files|grep enabled 查看启动失败的服务列表：systemctl –failed 配置firewalld-cmd 查看版本： firewall-cmd –version 查看帮助： firewall-cmd –help 显示状态： firewall-cmd –state 查看所有打开的端口： firewall-cmd –zone=public –list-ports 更新防火墙规则： firewall-cmd –reload 查看区域信息: firewall-cmd –get-active-zones 查看指定接口所属区域： firewall-cmd –get-zone-of-interface=eth0 拒绝所有包：firewall-cmd –panic-on 取消拒绝状态： firewall-cmd –panic-off 看是否拒绝： firewall-cmd –query-panic 开启指定端口呢 添加 firewall-cmd –zone=public –add-port=80/tcp –permanent （–permanent永久生效，没有此参数重启后失效） 重新载入 firewall-cmd –reload 查看 firewall-cmd –zone= public –query-port=80/tcp 删除 firewall-cmd –zone= public –remove-port=80/tcp –permanent]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot与Docker镜像的快速构建]]></title>
    <url>%2F2017%2F03%2F02%2Fspring-boot--e4-b8-8edocker-e9-95-9c-e5-83-8f-e6-9e-84-e5-bb-ba-e4-b8-80-e6-ad-a5-e5-88-b0-e4-bd-8d%2F</url>
    <content type="text"><![CDATA[在微服务场景下,SpringBoot和Docker镜像都具备”拆箱即用”的特点，将二者以maven插件进行结合,可以迅速构建出我们需要的微服务镜像，交由阿里云提供的镜像仓库集中管理分发。 准备工作 Docker环境 基于Maven的项目和docker-maven-plugin 阿里云私有仓库（registry.cn-hangzhou.aliyuncs.com）推荐用私网服务地址或VPC专用线路，更高效，且不耗费流量 熟悉Docker和Maven 1.配置pom.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576&lt;build&gt; &lt;plugins&gt; &lt;!--必须用springboot提供的编译插件，区别于传统maven插件，并且必须放在docker-maven-plugin的前面--&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.5.1.RELEASE&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt; &lt;version&gt;0.4.13&lt;/version&gt; &lt;!--执行maven clean install的时候会顺带执行下面三个步骤，依次是：编译、制作镜像、发布镜像到私仓--&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;build&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;tag-image&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;tag&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;image&gt;$&#123;docker.registry&#125;/$&#123;docker.prefix&#125;/$&#123;docker.base.profile&#125;:$&#123;project.build.finalName&#125;&lt;/image&gt; &lt;newName&gt;$&#123;docker.registry&#125;/$&#123;docker.prefix&#125;/$&#123;docker.base.profile&#125;:$&#123;project.build.finalName&#125;&lt;/newName&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;push-image&lt;/id&gt; &lt;phase&gt;deploy&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;push&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;imageName&gt;$&#123;docker.registry&#125;/$&#123;docker.prefix&#125;/$&#123;docker.base.profile&#125;:$&#123;project.build.finalName&#125;&lt;/imageName&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;!-- 私有仓库配置，需要settings.xml文件配合serverId对应的服务地址 --&gt; &lt;!--利用Maven提交服务密码加密的功能，添加镜像私仓的认证信息--&gt; &lt;serverId&gt;docker-aliyun&lt;/serverId&gt; &lt;registryUrl&gt;http://$&#123;docker.registry&#125;&lt;/registryUrl&gt; &lt;imageName&gt;$&#123;docker.registry&#125;/$&#123;docker.prefix&#125;/$&#123;docker.base.profile&#125;&lt;/imageName&gt; &lt;!--基础镜像如果本地不存在最好先手动pull一个，避免莫名其妙的问题--&gt; &lt;baseImage&gt;$&#123;docker.registry&#125;/$&#123;docker.prefix&#125;/$&#123;docker.base.profile&#125;:$&#123;docker.base.image&#125;&lt;/baseImage&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/&lt;/targetPath&gt; &lt;!--指定为/src/main/docker目录后jar包和生成的Dockerfile文件都会放到这里--&gt; &lt;directory&gt;$&#123;project.build.directory&#125;&lt;/directory&gt; &lt;include&gt;$&#123;project.build.finalName&#125;.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;exposes&gt; &lt;!--默认需要暴露的端口--&gt; &lt;expose&gt;8080&lt;/expose&gt; &lt;/exposes&gt; &lt;!--容器启动脚本--&gt; &lt;cmd&gt;[&amp;quot;java&amp;quot;, &amp;quot;-jar –spring.active.profile=$&#123;cfg_dir&#125;&amp;quot;, &amp;quot;/$&#123;project.build.finalName&#125;.jar&amp;quot;]&lt;/cmd&gt; &lt;imageTags&gt; &lt;!--docker的tag为项目版本号、latest--&gt; &lt;imageTag&gt;$&#123;project.artifactId&#125;&lt;/imageTag&gt; &lt;/imageTags&gt; &lt;!--install阶段也上传，否则只有deploy阶段上传--&gt; &lt;pushImage&gt;true&lt;/pushImage&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 2.settings.xml配置私有库的访问首先使用你的私有仓库访问密码生成主密码： 1mvn --encrypt-master-password &lt;password&gt; 其次在settings.xml文件的同级目录创建settings-security.xml文件，将主密码写入： 1234&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;settingsSecurity&gt; &lt;master&gt;&#123;asd7G5J?SD*****************=&#125;&lt;/master&gt;&lt;/settingsSecurity&gt; 最后使用你的私有仓库访问密码生成服务密码， 1mvn --encrypt-password &lt;password&gt; 将生成的密码写入到settings.xml的servers中: 12345678&lt;server&gt; &lt;id&gt;docker-aliyun&lt;/id&gt; &lt;username&gt;iwannarungg@gmail.com&lt;/username&gt; &lt;password&gt;&#123;D9YIyWYvtYsHayLjIenj***********=&#125;&lt;/password&gt; &lt;configuration&gt; &lt;email&gt;iwannarungg@gmail.com&lt;/email&gt; &lt;/configuration&gt; &lt;/server&gt; 更多settings.xml配置参考：http://maven.apache.org/ref/3.3.9/maven-settings/settings.html **注意:settings.xml和settings-security.xml必须放在~/.m2/路径下面，否则生成的私仓认证信息会无效 3.执行maven install如果&lt;pushimage&gt;false&lt;/pushimage&gt;则install阶段将不提交Docker镜像，只有maven的deploy阶段才提交。 1mvn clean install -Dmaven.test.skip=true -s ~/.m2/settings.xml 之后，镜像会被推送到阿里云镜像仓库中,镜像。 TIPS多环境镜像区分:真实开发中我们往往需要针对不同的环境设定不同的配置信息，需要一个一键切换环境的方式去构建不同环境版本的镜像。这里可以利用以下几个性质： - 如果Dockerfile文件需要maven构建参数（比如需要构建后的打包文件名等），则使用@@占位符（如@project.build.finalName@）原因是Sping Boot 的pom将resource插件的占位符由${}改为@@，非继承Spring Boot 的pom文件，则使用${}占位符。 - Spring Boot项目中yml配置文件读取pom.xml属性时也是通过@@占位符读取。另外spring.profile.active与Maven的profile可以很好的结合起来，以maven install -P [dev/test/pro]对应的激活spring.profile.active=[dev/test/pro]属性的目的。 具体实现：定义多个application-xxx.yml（比如测试环境可以是application-test.yml）,和一个公共的application.yml: 1234567spring: profiles: active: @maven_active_profile@ ...//这里还可以配置不同环境中公共部分配置，如服务暴露地址等sms_push_url: /sms/push ... 然后在pom.xml中 1234567891011121314&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;test&lt;/id&gt; &lt;properties&gt; &lt;maven_active_profile&gt;test&lt;/maven_active_profile&gt; &lt;/properties&gt; &lt;profile/&gt; &lt;profile&gt; &lt;id&gt;product&lt;/id&gt; &lt;properties&gt; &lt;maven_active_profile&gt;product&lt;/maven_active_profile&gt; &lt;/properties&gt; &lt;profile/&gt;&lt;profiles/&gt; 多环境镜像生成mvn clean install -P product就可以激活application.xml中spring.profile.active=product的配置,然后生成对应环境的镜像，再阿里云镜像仓库的namespace划分仓库，利用镜像的tag可以很好对镜像进行版本管理。]]></content>
      <tags>
        <tag>Docker</tag>
        <tag>Gradle</tag>
        <tag>Java</tag>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis消息通知之"发布/订阅"模式]]></title>
    <url>%2F2017%2F02%2F26%2Fdocker--e4-bf-ae-e6-94-b9-e9-bb-98-e8-ae-a4-e7-9a-84-e9-95-9c-e5-83-8f-e5-ad-98-e5-82-a8-e4-bd-8d-e7-bd-ae%2F</url>
    <content type="text"><![CDATA[Redis除了常见的任务队列和优先级队列之外，提供了一种类似JMS中”发布/订阅”的模式。这个模式中包含两个角色，包含发布者和订阅者。 订阅者可以订阅一个或多个(channel)，而发布者可以向指定的频道发送消息，所有订阅该频道的订阅者都会收到此消息。 发布者发布消息的命令是PUBLISH [channel] [message]12redis&gt;PUBLIC channel hello（Integer） 0 由于此时没有该频道的订阅者，所以返回信息为0。发出的消息不会被持久化，也就是后续才订阅这个channel频道的订阅者将不会收到本条信息。 订阅者执行SUBSCRIBE [channel...]可以订阅指定频道的消息1redis&gt;SUBSCRIBE channel 执行SUBSCRIBE命令后客户端会进入订阅状态，处于此状态的客户端(redis-cli)有且只能使用以下四个命令: SUBSCRIBE UNSUBSCRIBE PSUBSCRIBE PUNSUNSCRIBE 进入订阅状态后的客户端可能收到一下三种不同类型的回复。每种回复均包含3个值。 第一个值是消息的类型，根据消息类型的不同，第二、三值的含义也不同。 消息类型取值有一下三种: subscribe。第一个值表示订阅成功的反馈信息。第二个值是当前订阅频道的名字。第三个值表示当前客户端订阅的频道数。 message。这个类型的值是我们最关注的，表示接受到消息。第二个值标识产生消息的频道名称，第三个值是消息的内容。 unsubscribe。标识成功取消某个频道。第二个值是对应频道的名称，第三个是当前客户端订阅的频道数量，当该值为0时客户端就会退出订阅状态。之后就可以执行其他非”发布/订阅”模式的命令了。 按照规则订阅 为了更方便订阅多个频道还可以使用PSUBSCRIBE命令订阅指定的规则。1redis&gt;PSUBSCRIBE channel.?* channel.?可以匹配channel.1和channel.10，但不会匹配channel。 接受到消息后的回复信息是: “pmessage”(表示是通过PSUBSCRIBE命令订阅的频道) “channel.?”(标识订阅时使用的通配符) “channel.1”(标识实际接收到消息的频道命令) “hello”(表示消息的内容) 退订的命令是UNPSUBSCRIBE 省略则退订所有规则下的频道]]></content>
      <tags>
        <tag>NoSql</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot支持获取的属性源]]></title>
    <url>%2F2017%2F02%2F24%2Fspring-boot-e6-94-af-e6-8c-81-e8-8e-b7-e5-8f-96-e7-9a-84-e5-b1-9e-e6-80-a7-e6-ba-90%2F</url>
    <content type="text"><![CDATA[Spring Boot能从多种属性源获得属性 命令行参数 java:comp/env里的JNDI属性 JVM系统属性 操作系统环境变量 随机生成的带random.*前缀的属性（在设置其他属性时，可以引用它们，比如${random.long}） 应用程序以外的application.properties或者appliaction.yml文件 打包在应用程序内的application.properties或者appliaction.yml文件 通过@PropertySource标注的属性源 默认属性 这个列表按照优先级排序，也就是说，任何在高优先级属性源里设置的属性都会覆盖低优先级的相同属性。例如，命令行参数会覆盖其他属性源里的属性。 application.properties和application.yml文件能放在以下四个位置 外置，在相对于应用程序运行目录的/config子目录里。 外置，在应用程序运行的目录里。 内置，在config包内。 内置，在Classpath根目录。 同样，这个列表按照优先级排序。也就是说，/config子目录里的application.properties会覆盖应用程序Classpath里的application.properties中的相同属性。 此外，如果你在同一优先级位置同时有application.properties和application.yml，那么application.yml里的属性会覆盖application.properties里的属性。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录一次高并发情况下数据库读取数据异常]]></title>
    <url>%2F2017%2F02%2F15%2F-e8-ae-b0-e5-bd-95-e4-b8-80-e6-ac-a1-e9-ab-98-e5-b9-b6-e5-8f-91-e6-83-85-e5-86-b5-e4-b8-8b-e6-95-b0-e6-8d-ae-e5-ba-93-e8-af-bb-e5-8f-96-e6-95-b0-e6-8d%2F</url>
    <content type="text"><![CDATA[HQ项目小概率出现的问题，在此记录： 在极短的时间内(10ms),连续提交3个事务（读+写） 第一个事务读取到数值a=10，执行更新操作（+1），a=11，事务提交 5ms后，第二个事务读取到数值还是a=10（理论上应该是a=11），执行更新操作（+1），a=11，事务提交 7ms后，第三个事务读取到数值为a=11，执行更新操作（+1），a=12，事务提交 以上操作均有zookeeper锁+spring事务，mysql事务级别为READCOMMITTED 解决过程: 为了复现这个异常现象在测试环境做了以下几个事情： 开启白银品种周末禁止下单的限制，达到下单失败但是会冻结资金的目的。(模拟生产环境平仓解冻资金环节); 修改一次下单连续提交五次资金冻结操作。（模拟生产环境用户使用”闪电平仓”功能，一次操作连续发起多次解冻资金操作）; 使用JMeter进行高并发操作:5秒内启动完100个线程去请求下单接口，模拟100个用户在短时间内下单操作 另外还做了几个对照组： 1个线程，循环执行100次； 10个线程，循环执行10次； 100个线程，循环执行1次； 复现结果： Clipboard Image.png(此处敏感省略) 修复过程： 123451.显式关闭mysql缓存策略。------&gt;无效2.对事务和aop实现的zk锁优先级进行了调整。-------&gt;无效3.对被调用方法的事务传播行为进行调整。 --------&gt;有效 事务传播行为调整的详细说明： 假设methodB是真正执行资金变更的方法， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748ServiceA &#123; @Transactional(rollbackFor = RuntimeException.class) @UserSensitive(value=1,cash=&quot;cash&quot;) void methodA() &#123; ... //一些封装数据操作，作为参数传递给methodB() ... ServiceB.methodB(); &#125; &#125; ServiceB &#123; void methodB() &#123; //1.查询资金余额 ... //2.更新资金余额 &#125; &#125; 修改后的代码：ServiceA &#123; @Transactional(rollbackFor = RuntimeException.class) @UserSensitive(value=1,cash=&quot;cash&quot;) void methodA() &#123; ... //一些封装数据操作，作为参数传递给methodB() ... ServiceB.methodB(); &#125; &#125; ServiceB &#123; @Transactional(rollbackFor = RuntimeException.class,propagation = Propagation.REQUIRES_NEW) void methodB() &#123; //1.查询资金余额 ... //2.更新资金余额 &#125; &#125; 方法B添加事务并指定传播级别：PROPAGATION_REQUIRES_NEW当执行到ServiceB.methodB的时候，ServiceA.methodA所在的事务就会挂起，ServiceB.methodB会起一个新的事务，等待ServiceB.methodB的事务完成以后，他才继续执行。如此一来，确保了方法B事务在提交后才会继续下一步操作。 他与PROPAGATION_REQUIRED (默认事务)的事务区别在于事务的回滚程度。因为ServiceB.methodB是新起一个事务，那么就是存在两个不同的事务。如果ServiceB.methodB已经提交，那么ServiceA.methodA失败回滚，ServiceB.methodB是不会回滚的。如果ServiceB.methodB失败回滚，如果他抛出的异常被ServiceA.methodA捕获，ServiceA.methodA事务仍然可能提交。]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jenkins容器中使用docker]]></title>
    <url>%2F2017%2F01%2F22%2Fjenkins-e5-ae-b9-e5-99-a8-e4-b8-ad-e4-bd-bf-e7-94-a8docker%2F</url>
    <content type="text"><![CDATA[在容器中运行docker命令有两种方式： DinD（Docker-in-Docker）：在容器中安装一个完整的隔离的docker环境； DooD（Docker-outside-of-Docker）：通过加载宿主Docker socket和程序达成重用宿主镜像的目的； ps: DooD比DinD简单得多（至少在配置方面），尤其是能重用并缓存宿主上的镜像。反之，如果你想实现镜像对宿主的隐藏和隔离，则最好使用DinD。 这里举个DooD的栗子，因为较为简单，也适用于CI容器： 我们使用官方Jenkins镜像作为基础，赋予jenkins用户sudo权限以便能在容器内运行Docker命令 12345678FROM jenkins:1.596USER rootRUN apt-get update \ &amp;&amp; apt-get install -y sudo \ &amp;&amp; rm -rf /var/lib/apt/lists/*RUN echo &quot;jenkins ALL=NOPASSWD: ALL&quot; &gt;&gt; /etc/sudoersUSER jenkins 现在来构建并运行容器，将Docker socket和程序映射进来。 1234$ docker build -t myjenk ....Successfully built 471fc0d22bff$ docker run -d -v /var/run/docker.sock:/var/run/docker.sock -v $(which docker):/usr/bin/docker -p 8080:8080 myjenk 容器跑起来之后，通过docker inspect``命令，可以查看到其中信息： 123456789··· &quot;HostConfig&quot;: &#123; &quot;Binds&quot;: [ &quot;/var/run/docker.sock:/var/run/docker.sock&quot;, &quot;/bin/docker:/usr/bin/docker&quot;, &quot;/data/cloud_volumes/:/var/jenkins_home&quot; ] &#125;··· 之后就可以在jenkins容器中愉快的使用docker命令，构建我们ci过程中需要的镜像了。]]></content>
      <tags>
        <tag>Docker</tag>
        <tag>Jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[log4j-over-slf4j 与 slf4j-log4j12 冲突]]></title>
    <url>%2F2017%2F01%2F20%2Flog4j-over-slf4j--e4-b8-8e-slf4j-log4j12--e5-86-b2-e7-aa-81%2F</url>
    <content type="text"><![CDATA[log4j-over-slf4j ： log4j1切换到slf4j slf4j-log4j12 : slf4j切换到log4j1 如果这两者共存的话，必然造成相互委托，造成内存溢出。但是log4j-over-slf4内部做了一个判断，可以防止造成内存溢出：即判断log4j-over-slf4j jar包中的org.slf4j.impl.Log4jLoggerFactory是否存在，如果存在则表示冲突了，抛出异常提示用户要去掉对应的jar包，代码如下，在slf4j-log4j12 jar包的org.apache.log4j.Log4jLoggerFactory中： 123456789101112131415161718192021222324252627282930class Log4jLoggerFactory &#123; private static Hashtable log4jLoggers = new Hashtable(); private static final String LOG4J_DELEGATION_LOOP_URL = &quot;http://www.slf4j.org/codes.html#log4jDelegationLoop&quot;; Log4jLoggerFactory() &#123; &#125; public static synchronized Logger getLogger(String name) &#123; if(log4jLoggers.containsKey(name)) &#123; return (Logger)log4jLoggers.get(name); &#125; else &#123; Logger log4jLogger = new Logger(name); log4jLoggers.put(name, log4jLogger); return log4jLogger; &#125; &#125; static &#123; try &#123; Class.forName(&quot;org.slf4j.impl.Log4jLoggerFactory&quot;); String e = &quot;Detected both log4j-over-slf4j.jar AND slf4j-log4j12.jar on the class path, preempting StackOverflowError. &quot;; String part2 = &quot;See also http://www.slf4j.org/codes.html#log4jDelegationLoop for more details.&quot;; Util.report(e); Util.report(part2); throw new IllegalStateException(e + part2); &#125; catch (ClassNotFoundException var2) &#123; ; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>日志收集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo在docker容器中遇到的跨主机通信问题]]></title>
    <url>%2F2016%2F12%2F21%2Fdubbo-e5-9c-a8docker-e5-ae-b9-e5-99-a8-e4-b8-ad-e9-81-87-e5-88-b0-e7-9a-84-e8-b7-a8-e4-b8-bb-e6-9c-ba-e9-80-9a-e4-bf-a1-e9-97-ae-e9-a2-98%2F</url>
    <content type="text"><![CDATA[前言 最近搞Docker化的持续继承，项目中使用到Docker和Dubbo，想在Docker中运行一个服务并把该服务自身的信息发布到Dubbo注册中心。 刚开始测试时候将所有容器都放在同一台主机中，测试过程很顺利，但是当进行Docker主机扩展，将容器部署在不同的主机时候，就发现一个奇怪的现象：应用之间调试不通了。 这里对具体问题解释一下：Dubbo提供了一个方便的服务发现机制，每个服务（这里称为提供者）只要向Dubbo注册中心注册过，注册中心就会将服务的地址发送给同样在注册中心注册的服务调用方（这里称为消费者），之后即使Dubbo注册中心挂了也不影响服务的调用。 当服务提供者部署在容器中时，这时候发现其在Dubbo中心注册的是容器的IP地址，而对于另外一台主机上的消费者来说这个IP是不可访问的，当时我在网上找了很多相关的资料，最后得出几种解决方案。 解决方案 设置容器的IP与主机IP在同一网段内，使容器IP可直接访问(会占用大量的IP地址，且IP会限制在同一网段，在生产环境中往往不可能)。 通过复杂的iptables路由规则，通过多层桥接方式打通网络(此法是可行的，也是今后要考虑的，但是操作起来略麻烦)。 对Dubbo进行扩展，扩展dubbo protocol配置，增加配置项publish host、 publish port，对应主机的ip和port，并且在注册服务时将主机的ip和port写到注册中心。（这种方法需要对Dubbo进行扩展，不太建议） 以上三种方法都有一定的局限性和复杂性，我就想能否有更加简单便捷的方法可以解决这个问题。 最后查阅资料时发现Dubbo是先通过hostname（通过ping hostname）来得到本机的IP地址的，换句话说也就是dubbo是读取/etc/hosts文件得到主机的IP的，又因为Docker在重启时候IP地址会重置，即/etc/hosts文件在Docker重启时候会被重置，所以考虑在Docker启动的脚本中动态修改/etc/hosts文件，从而改变Docker主机的IP。 最终的处理方案在启动Docker时候将宿主机的IP作为一个环境变量传递给Docker主机，然后Docker主机中将运行上面的start.sh脚本，该脚本的作用是在启动服务之前，通过查找/etc/hosts文件把现有的Docker主机名对应的IP修改为宿主机的IP，从而使得注册到Dubbo中心的是宿主机的IP和端口。init.sh 12345#!/bin/shcp /etc/hosts /etc/hosts.tempsed -i &quot;s/.*$(hostname)/$DOCKER_IP $(hostname)/&quot; /etc/hosts.tempcat /etc/hosts.temp &gt; /etc/hosts/data/server/tomcat/bin/catalina.sh run 将这段脚本放入Dockerfile的CMD参数中，作为启动指令： 1234567FROM myregistry:basic-imagesRUN rm -rf /data/server/tomcat/webapps/*COPY target/futuresquota/futuresquota.war /data/server/tomcat/webappsVOLUME /data/server/tomcat/logsVOLUME /data/server/logsEXPOSE 8080CMD [ &quot;/data/init.sh&quot;, &quot;run&quot; ]]]></content>
      <tags>
        <tag>Docker</tag>
        <tag>Dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DevOps之路:Jenkins与Docker的结合]]></title>
    <url>%2F2016%2F12%2F21%2Fdevops-e4-b9-8b-e8-b7-afjenkins-e4-b8-8edocker-e7-9a-84-e7-bb-93-e5-90-88%2F</url>
    <content type="text"><![CDATA[最近两周正巧遇上新项目上线，跟老大提了一下这次想用docker容器部署的想法，简单交流一下就开始着手实施了。 回顾一下，之前为公司搭的CI和CD平台一直停留在比较传统的部署方式，虽然短期内足够应付期货业务的部署任务，不过长远的看来这种方式并不是最佳的。从消耗资源来看，每次集中部署时jenkins的主节点和代理节点之间传输war包需要消耗大量流量，除此之外，倘若主节点和代理节点不在一个网段，传输效率十分低下。从部署环节来看，前期做的规划不够，jenkin的任务设计的比较粗糙简单且复用性低。 Docker技术在近几年发展迅速，具有诸多优点： 更高效的利用系统资源 更快速的启动时间 一致的运行环境 更轻松的迁移 ······ 难怪它倍受运维人员的喜爱。 如果将Jenkins与Docker结合，我的设想大概如下： 实现docker化的持续集成和持续部署 对开发和运维（DevOps）人员来说，最希望的就是一次创建或配置，可以在任意地方正常运行。 使用 Docker 可以通过定制应用镜像来实现持续集成、持续交付、部署。开发人员可以通过 Dockerfile 来进行镜像构建，并结合 持续集成(Continuous Integration) 系统进行集成测试，而运维人员则可以直接在生产环境中快速部署该镜像，甚至结合 持续部署(Continuous Delivery/Deployment) 系统进行自动部署。 而且使用 Dockerfile使镜像构建透明化，不仅仅开发团队可以理解应用运行环境，也方便运维团队理解应用运行所需条件，帮助更好的生产环境中部署该镜像。 前提和准备工作 gitlab代码仓库管理源码，且为maven项目通过profile进行环境区分； 一台Jenkins主服务节点（4核8g），N台jenkins代理节点slave(同时用作生产环境)，各服务器最好处于同一个内网; centOS环境、maven环境、docker环境、jdk等； 阿里云docker私有仓库(内网消耗流量)； Jenkins docker插件，推荐的有：Docker Commons Plugin、docker-build-step 其余的插件如CloudBees Docker系列应该也不错，只是在我的版本上似乎不好使；其他用到的插件，如：maven plugin、pipeline pipeline、Environment Injector Plugin、Email Extension Template Plugin、gitlab plugin等； 集成jdk、tomcat的基础镜像，后续我们的服务基于这个basic image构建服务镜像，然后在指定节点处运行这些镜像，产生服务； ps:这里基础镜像我选的是官方的CentOS7+jdk1.8,然后自己运行起来后往里面装tomcat8，并配置好时区，中文编码等问题，再根据服务产生的日志规划好空间结构，利用docker commit的命令重新的自定义版基础镜像； Jenkins配置首先，job可以分为三部分： 第一部，分通过Maven插件专门负责打包，并将产物放到指定目录； 123456789101112#!/bin/bashDIRECTORY=&quot;$dockerFilePath/target&quot;if [ &quot;`ls -A $DIRECTORY`&quot; = &quot;&quot; ]; thenecho &quot;$DIRECTORY is indeed empty&quot;else rm -rf $DIRECTORY/**/*.warfiecho &quot;copy all war to targetPath:$DIRECTORY&quot;cp /var/jenkins_home/workspace/checkoutCode/stock-parent/*/target/*.war $dockerFilePath/target/echo &quot;remove all none images&quot;#登录私仓，为后面push镜像做好准备docker login -u $dockerAccount -p $dockerPwd $registryUrl 这里大量运用了环境变量以达到参数化构建任务的目的，做到最大程度复用任务，上面的代码将打包完成后的war包全部复制移动到放有Dockerfile的target目录，目的是为了之后制作镜像； 第二部分，将上个环节产生的war包按照服务名称分类，通过脚本生成对应的Dockerfile，然后build成镜像，push到阿里云私仓库； 12345678910111213141516#!/bin/bash#definedDOCKERFILE_PATH=$dockerFilePath/target/$engine/DockerfileTARGET_PATH=&quot;COPY target/$engine/$engine.war /data/server/tomcat/webapps &quot;set -eecho &quot;clear Dockerfile：$TARGET_PATH&quot;: &gt; $DOCKERFILE_PATHcp /var/jenkins_home/workspace/$prepareJob/stock-parent/*$engine*/target/*.war $dockerFilePath/target/$engine/echo &quot;FROM $mainRegistry/$profile:$basicImage&quot; &gt;&gt; $DOCKERFILE_PATHecho &quot;RUN rm -rf /data/server/tomcat/webapps/* &quot;&gt;&gt; $DOCKERFILE_PATHecho $TARGET_PATH &gt;&gt; $DOCKERFILE_PATHecho &quot;VOLUME /data/server/tomcat/logs&quot; &gt;&gt; $DOCKERFILE_PATHecho &quot;VOLUME /data/server/logs&quot; &gt;&gt; $DOCKERFILE_PATHecho &quot;EXPOSE 8080&quot; &gt;&gt; $DOCKERFILE_PATHecho &quot;CMD [ \&quot;/data/start.sh\&quot;, \&quot;run\&quot; ]&quot; &gt;&gt; $DOCKERFILE_PATHecho &quot; Dockerfile init done!&quot; ps: 这里指定了默认的映射端口和日志挂载路径以及事前设置好的tomcat启动脚本； 然后，利用docker插件，配置好所需要的参数，包括docker repository name和Docker registry URL等信息。 这里插入一下，docker仓库的路径也需要事先规划好，可以参考域名空间的概念对仓库按照服务划分； 在job的构建末尾，还可以发送任务构建结果的告知邮件通知到我们： 第三部分，为具体的任务打好label标签，这里的标签信息确保该任务只会在拥有这个标签的子节点(slave)上执行； 支持运算符过滤标签： 这里有个坑，事实上，这里的label如果同时填写了多个标签，并不意味着这些具有这些标签的slave都会执行。而是根据每个节点的当前的富余执行能力来分配（最大并发执行能力取决于事先未节点分配的 of executors属性）。比方说，你的label处填写了a ，并不意味着所有具有a标签的节点都会执行当前的job，只要其中一个具有a标签的节点处于空闲状态就会直接执行，而处于忙碌状态的节点会直接略过。 拉取镜像，运行镜像: 12345678#!/bin/bash#登录私有仓库docker login -u $dockerAccount -p $dockerPwd $registryUrl#拉取镜像docker pull $mainRegistry/$profile:$engine-$tagVersiondocker rm -f $engine || true#运行容器docker run -d --name $engine -e DOCKER_IP=$DOCKER_IP $logMapping $sysLogMapping $defaultPortMapping -p 20003:8080 -p 20803:20803 $mainRegistry/$profile:$engine-$tagVersion ps:$logMapping $sysLogMapping可以通过全局的环境变量设置，配合上创建job时支持复制创建，能提高配置的灵活性； 最后，通过pipeline的方式将这些任务串行或并行的连接起来： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394node(&apos;master&apos;)&#123; stage &apos;checkout code&amp;package&apos; //build &apos;checkoutCode&apos;&#125; stage &apos;deplody market &amp;&amp; futuresquota &amp;&amp; sms &amp;&amp; financy&apos; parallel createImgOfMarket: &#123; build &apos;mk-market&apos; node(&apos;sdb_market_1&apos;)&#123; build &apos;pb-market-1&apos; &#125; node(&apos;sdb_market_2&apos;)&#123; build &apos;pb-market-2&apos; &#125; &#125;, createImgOfFuturesquota:&#123; build &apos;mk-futuresquota&apos; node(&apos;sdb_quota_1&apos;)&#123; build &apos;pb-futuresquota&apos; &#125; &#125;,createImgOfSms:&#123; build &apos;mk-sms&apos; node(&apos;sdb_sms_1&apos;)&#123; build &apos;pb-sms-1&apos; &#125; node(&apos;sdb_sms_2&apos;)&#123; build &apos;pb-sms-2&apos; &#125; &#125;,createImgOfFinancy:&#123; build &apos;mk-financy&apos; node(&apos;sdb_financy_1&apos;)&#123; build &apos;pb-financy-1&apos; &#125; node(&apos;sdb_financy_2&apos;)&#123; build &apos;pb-financy-2&apos; &#125; &#125; failFast: true|false stage &apos;deplody order &amp;&amp; promotion &amp;&amp; user &apos; parallel createImgOfOrder:&#123; build &apos;mk-order&apos; node(&apos;sdb_order_1&apos;)&#123; build &apos;pb-order-1&apos; &#125; node(&apos;sdb_order_2&apos;)&#123; build &apos;pb-order-2&apos; &#125; &#125;, createImgOfpromotion:&#123; build &apos;mk-promotion&apos; node(&apos;sdb_promotion_1&apos;)&#123; build &apos;pb-promotion-1&apos; &#125; node(&apos;sdb_promotion_2&apos;)&#123; build &apos;pb-promotion-2&apos; &#125; &#125;,createImgOfUser:&#123; build &apos;mk-user&apos; node(&apos;sdb_user_1&apos;)&#123; build &apos;pb-user-1&apos; &#125; node(&apos;sdb_user_2&apos;)&#123; build &apos;pb-user-2&apos; &#125; &#125; failFast: true|false stage &apos;deplody risk &amp;&amp; task &amp;&amp; activity &apos; parallel createImgOfRisk: &#123; build &apos;mk-risk&apos; node(&apos;sdb_risk_1&apos;)&#123; build &apos;pb-risk&apos; &#125; &#125;, createImgOfTask:&#123; build &apos;mk-task&apos; node(&apos;sdb_task_1&apos;)&#123; build &apos;pb-task-1&apos; &#125; &#125;,createImgOfActivity:&#123; build &apos;mk-activity&apos; node(&apos;sdb_activity_1&apos;)&#123; build &apos;pb-activity-1&apos; &#125; node(&apos;sdb_activity_2&apos;)&#123; build &apos;pb-activity-2&apos; &#125; &#125; failFast: true|false 最后的执行效果（使用了最新的视图插件blue ocean）：]]></content>
      <tags>
        <tag>Jenkins</tag>
        <tag>持续集成</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 直接关闭ssh窗口和exit的区别]]></title>
    <url>%2F2016%2F12%2F14%2Flinux--e7-9b-b4-e6-8e-a5-e5-85-b3-e9-97-adssh-e7-aa-97-e5-8f-a3-e5-92-8cexit-e7-9a-84-e5-8c-ba-e5-88-ab%2F</url>
    <content type="text"><![CDATA[昨天因为业务方的疏忽没有及时续费阿里云导致六台服务器等待释放…直接导致所有生产服务宕机…… 在使用jenkins恢复所有服务的时候，遇到了一点坑，在这里记录一下： 由于jenkins代理节点需要事先以 1java -jar slave.jar -jnlpUrl http://xxx.xxx.xxx/computer/xxx_market_promotion_sms/slave-agent.jnlp -secret 2275ca9cb495278ce3d0e22e1b416679ef4ae71458f4080c3823457b271363e8 &amp; 运行jar的方式启动后台进程，之后再在jenkins控制台运行job，问题出现在完成部署任务后直接将所有启动好slave.jar的ssh终端窗口直接点了关闭，导致所有slave启动的java服务随着窗口关闭，进程自动终止了，更可怕的是这个问题第二天才发现…还好这个版本用户量还不大。 总结一下, 描述： 场景1：ssh登录机器，通过添加（&amp;），启动任务到后台，通过exit命令退出，任务依然存在。 场景2：ssh登录机器，通过添加（&amp;），启动任务到后台，直接关闭ssh终端（GUI 直接叉掉窗口），任务终止。 原因是当系统默认huponexit 为off时，exit时不会向终端所属任务发SIGHUP信号，所以exit后任务依然存在， 如果激活该选项： 1shopt -s huponexit 重复进行上述测试，exit时后台进程退出]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker容器迁移]]></title>
    <url>%2F2016%2F12%2F06%2Fdocker-e5-ae-b9-e5-99-a8-e8-bf-81-e7-a7-bb%2F</url>
    <content type="text"><![CDATA[12最近没啥安全感，对服务器也是（其实是担心花费了一些心血搭起的服务受不可抗拒的因素挂了，重新搭起来又需要消耗时间)，于是倒不如趁有空的时候想想方案，有备无患。其实灵活、易迁移本来就是docker的特性，特别适用于生成交付过程、系统升级或服务器迁移，能够以极小的代价和极高的效率完成&quot;克隆&quot;。 主要的迁移方式有：容器迁移和镜像迁移 首先是基于容器的迁移： 导出容器 如果要导出本地某个容器，可以使用docker export 命令。 1234$ sudo docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES7691a814370e ubuntu:14.04 &quot;/bin/bash&quot; 36 hours ago Exited (0) 21 hours ago test$ sudo docker export 7691a814370e &gt; ubuntu.tar 这样将导出容器快照到本地文件。然后，这份文件可以随意迁移其他环境中去,再通过以下的介绍进行恢复。 导入容器快照1234$ cat ubuntu.tar | sudo docker import - test/ubuntu:v1.0$ sudo docker imagesREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZEtest/ubuntu v1.0 9d37a6082e97 About a minute ago 171.3 MB 此外，也可以通过指定 URL 或者某个目录来导入，例如 1$sudo docker import http://example.com/exampleimage.tgz example/imagerepo 基于镜像的迁移方式： 导出和导入镜像导出镜像docker save命令用于持久化镜像（不是容器） 1$ sudo docker save imagesname &gt; /home/save.tar 导入镜像1$ sudo docker load -i /home/save.tar 注意：用户既可以使用docker load来导入镜像存储文件到本地镜像库，也可以使用docker import来导入一个容器快照到本地镜像库。这两者的区别在于容器快照文件将丢弃所有的历史记录和元数据信息（即仅保存容器当时的快照状态），而镜像存储文件将保存完整记录，体积也要大。此外，从容器快照文件导入时可以重新指定标签等元数据信息。 简而言之，export 是不能回滚以前的操作的 save 是完整的导出，可以和以前的镜像那样，可以回滚操作。]]></content>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac下iTerm、zsh、autojump的完美结合]]></title>
    <url>%2F2016%2F12%2F03%2Fmac-e4-b8-8biterm-e3-80-81zsh-e3-80-81autojump-e7-9a-84-e5-ae-8c-e7-be-8e-e7-bb-93-e5-90-88%2F</url>
    <content type="text"><![CDATA[安装zsh: Mac预装了zsh，通过cat /etc/shells可以查看: 123456/bin/bash/bin/csh/bin/ksh/bin/sh/bin/tcsh/bin/zsh linux: 可以通过sudo yum install zsh 或 sudo apt-get install zsh来进行安装 安装完成后设置当前用户使用 zsh：chsh -s /bin/zsh，根据提示输入当前用户的密码就可以了。 &gt; 国外有个穷极无聊的程序员开发出了一个能够让你快速上手的zsh项目，叫做「oh my zsh」，Github 网址是：https://github.com/robbyrussell/oh-my-zsh。这玩意就像「X天叫你学会 C++」系列，可以让你神功速成，而且是真的。 安装oh my zsh -自动安装：wget https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh -O - | sh -手动安装: 12git clone git://github.com/robbyrussell/oh-my-zsh.git ~/.oh-my-zshcp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc 安装完毕，对iTerm2进行终端指定 新建一个终端页面，即可显示效果: - 安装autojump插件 mac:brew install autojump 执行：1[[ -s /root/.autojump/etc/profile.d/autojump.sh ]] &amp;&amp; source /root/.autojump/etc/profile.d/autojump.sh linux:git clone git://github.com/joelthelion/autojump.git ##本地克隆完成后进入目录，执行./install.py 然后，将 粘贴到~/.zshrc中，在plugin中开启autojumpsource命令执行一下，使其生效。 zsh的爆炸功能演示 首先是『别名』功能，内置的git命令别名功能，搭配oh my zsh的主题，相当赞: ps:这里使用quick-look预览命令查看文本，按住ctrl，将鼠标移到路径上会出现超链接，点击也能达到同样效果： 1. autojump 提供的j命令:]]></content>
      <tags>
        <tag>效率工具</tag>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用Alfred Workflow快速上传粘贴板图片至七牛图床并在Markdown中引用]]></title>
    <url>%2F2016%2F12%2F02%2F-e5-88-a9-e7-94-a8alfred-workflow-e5-bf-ab-e9-80-9f-e4-b8-8a-e4-bc-a0-e7-b2-98-e8-b4-b4-e6-9d-bf-e5-9b-be-e7-89-87-e8-87-b3-e4-b8-83-e7-89-9b-e5-9b-be%2F</url>
    <content type="text"><![CDATA[应用场景 Markdown最大的缺陷就是不能方便的在文章中插入本地图片，所以通常情况下我们需要一个好的图床来帮助我们，国内现在使用体验最佳的图床就是七牛云存储，但是为了插入一张图片我们通常需要做的事情是： 截个图存在桌面 - 打开七牛云存储的网站，上传图片 - 复制图片地址 - 在Markdown中使用语法调用图片插入 - 填写图片地址 - 如果你使用原图保护功能，还需要自己添加样式符 为了一张插入图片，真是心力交瘁。如果插入量巨大，真是不堪重负。所以我们需要使用一个工作流来一键帮我们完成这些复杂的机械化的工作。本教程实现的目标是- 截图到粘贴板 快捷键插入到Markdown文本 中途甚至不需要任何与编辑文本无关的工作，让你专心写作！ 教程准备工作 因为本方法用到了Alfred的PowerPackage扩展功能，所以你需要首先内（po）购（jie）其高级功能。 然后另一个准备工作就是七牛云存储了，到七牛云存储的官网注册一个账号，开始使用。关于七牛云存储的一些设置，在这里我想说一下： 七牛云存储的设置 在你的空间的数据处理中应该配置好「图片样式」：新建图片样式。 使用图片样式的好处是你可以根据需求插入不同大小的图片，毕竟Markdown是没有图片编辑和调整功能的。我设置了如下的样式： 这样我可以通过样式分割符合样式-480.jpg、-960.jpg、-1920.jpg调用不同大小的图片插入到文章中了。 配置七牛云存储文件同步 QRSync是七牛云存储提供的同步上传客户端工具，可以用于Linux、Mac OS X、Windows等操作系统。使用QRSync，可将用户本地某个目录的所有文件同步上传到七牛云存储中，同时支持增量上传，可以只将目录中新增的文件上传至七牛云存储。 首先，下载QRSBox Mac OS X: qrsynccli darwin_amd64 Linux 64bits: qrsyncli linux_amd64 Linux 32bits: qrsyncli linux_386 然后在你希望同步的文件夹下创建以下两个目录 CLI ： 用于存放QRSync命令脚本 Data ： 用于存放需要同步的图片、文件等 把下载好的QRSync解压后所有文件放到CLI目录下，在CLI目录下新建conf.json文件包含以下内容： 12345&#123; &quot;src&quot;: &quot;/home/your/sync_dir/Data&quot;, &quot;dest&quot;: &quot;qiniu:access_key=&lt;AccessKey&gt;&amp;secret_key=&lt;SecretKey&gt;&amp;bucket=&lt;Bucket&gt;&amp;key_prefix=&lt;KeyPrefix&gt;&quot;, &quot;debug_level&quot;: 1&#125; 其中： src是本地的同步目录Data，该目录下的文件会随时同步上传至七牛云存储。 AccessKey 和 SecretKey 需要在七牛云存储平台上申请 开通七牛开发者帐号 登录七牛开发者平台，查看 Access Key 和 Secret Key``&lt;bucket&gt;&lt;/bucket&gt; 是保存同步文件的资源空间名。 &lt;keyprefix&gt;&lt;/keyprefix&gt; 是文件前缀，可选。如果设置了该参数，那么上传的文件名前都会加上前缀。这个前缀主要用于在空间中区分不同上传来源的文件。 配置完成后，在CLI目录下就可以随时使用如下命令来同步文件夹了： 1./qrsync conf.json 另一个可以使用的工具是QRSBox，其支持后台实时监控目录，当有新文件加入到同步目录的时候，就会自动上传到相应的空间。但截止到我测试时（2016年3月19日）QRSBox仍有一些Bug导致工作流无法工作，所以我选择了更加可靠的QRSync。 配置WorkFlow 下面我们来配置WorkFlow工作流来让一切变得自动化起来，首先打开Alfred，进入Workflow，并且创建一个空白的工作流： 然后添加一个热键，我选择的是⌘+⇧+V，这样和我的Annotate截屏快捷键⌘+⇧+A正好形成一对： 然后添加一个action，选择osascript作为脚本语言 我们添加如下脚本： 123456789101112131415161718192021222324252627282930313233343536373839&lt;br&gt;&lt;/br&gt;property fileTypes : &#123;¬ &#123;«class PNGf», &quot;.png&quot;&#125;, ¬ &#123;JPEG picture, &quot;.jpg&quot;&#125;&#125;on getType() repeat with aType in fileTypes repeat with theInfo in (clipboard info) if (first item of theInfo) is equal to (first item of aType) then return aType end repeat end repeat return missing valueend getTypeset theType to getType()if theType is not missing value then set filePath to &quot;/Users/quentin/Documents/Qiniu/Data/&quot; --这里换成你自己放置图片的路径 set fileName to do shell script &quot;date \&quot;+%Y%m%d%H%M%S\&quot; | md5&quot; --用当前时间的md5值做文件名 if fileName does not end with (second item of theType) then set fileName to (fileName &amp; second item of theType as text) set markdownUrl to &quot;![](http://7xin49.com1.z0.glb.clouddn.com/&quot; &amp; fileName &amp; &quot;-960.jpg)&quot; --这里是你的七牛域名和设置的图片样式 set filePath to filePath &amp; fileName try set imageFile to (open for access filePath with write permission) set eof imageFile to 0 write (the clipboard as (first item of theType)) to imageFile close access imageFile set the clipboard to markdownUrl try tell application &quot;System Events&quot; keystroke &quot;v&quot; using command down end tell end try do shell script &quot;/Users/quentin/Documents/Qiniu/CLI/qrsync /Users/quentin/Documents/Qiniu/CLI/conf.json&quot; --此处是你的QRSync脚本目录和命令 on error try close access imageFile end try return &quot;&quot; end tryelse return &quot;&quot;end if 代码中有4处需要替换修改地方： 1set filePath to &quot;/Users/quentin/Documents/Qiniu/Data/&quot; 换成你自己设定的同步目录。 1set markdownUrl to &quot;![](http://7xin49.com1.z0.glb.clouddn.com/&quot; &amp; fileName &amp; &quot;-960.jpg)&quot; 设定成你的七牛空间域名： 注意如果你在设置QRSync时预设了前缀，记得在域名后面补上。另外再加上自己设定的图片样式-960.jpg，我在样式中自带了一个统一的文件后缀.jpg是为了让Markdown编辑器知道这是一个图片链接。 1do shell script &quot;/Users/quentin/Documents/Qiniu/CLI/qrsync /Users/quentin/Documents/Qiniu/CLI/conf.json&quot; 换成你的QRSync的命令脚本目录即可。 然后，添加一个通知，让我们得到上传成功的反馈： 最后，把Trigger、Action和Notification用线连起来，就大功告成了： 现在，你只需要 额外的同步工作流 此外，我再提供一个额外的工作流来负责同步目录到七牛云存储，针对那些直接复制到同步目录下的文件。另建一个Workflow，如图所示： 使用你喜欢的HotKey来启动上传，然后使用bash语言键入以下script： 1/Users/quentin/Documents/Qiniu/CLI/qrsync /Users/quentin/Documents/Qiniu/CLI/conf.json 同样，把相应目录改成你自己的QRSync命令脚本目录就可以了。 注意：当上一个工作流没有成功上传时，可以使用这个工作流再次上传。 直接下载 我把自己写好的WorkFlow上传到百度云了，你也可以自己下载，然后修改参数直接使用，传送门： 百度网盘下载 密码：q3cd 原文连接:https://www.zybuluo.com/fyywy520/note/317999#额外的同步工作流]]></content>
      <tags>
        <tag>效率工具</tag>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用七牛CND配置WordPress博客的静态资源加速]]></title>
    <url>%2F2016%2F12%2F02%2F-e6-b5-8b-e8-af-95-e5-9b-be-e7-89-87-e6-b0-b4-e5-8d-b0-e9-85-8d-e7-bd-ae%2F</url>
    <content type="text"><![CDATA[为了加快访问博客的速度，特意安装了一个七牛云存储的插件，主要用到了静态资源CDN加速和图片镜像到七牛云的功能。最后达到的效果是:后台的媒体资源路径是本博客域名加后缀构成的路径blog.iwannarun.cn/xxx.jpg，而在前台显示则为预设的pic.iwannarun.cn。通过后者访问图片，css和js等静态资源都会被七牛融合CDN加速。 这里大致记录一下过程： 从七牛控制台获取A/SKey、对象空间名称以及七牛上绑定的域名填写到指定位置: 填写博客域名，勾选需要的功能后博客这边就算配置完成了。 再次回到七牛控制台，进行镜像存储配置，对博客上所有新增资源文件同步到七牛云： 接着配置融合CDN功能，填写静态资源专用域名: 配置完成后，会提供cname域名： 等待七牛后台刷新后，将返回的cname拿到域名提供方的解析控制台，参考官方提供的方法添加cname记录： 效果 网页源代码的图片路径已经改变： 网页源代码的JS/CSS也被替换为七牛镜像存储： 在七牛控制台也能够看到CDN流量说明成功了]]></content>
      <tags>
        <tag>效率工具</tag>
        <tag>WordPress</tag>
        <tag>图床</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty通信框架学习笔记二]]></title>
    <url>%2F2016%2F11%2F30%2Fnetty-e9-80-9a-e4-bf-a1-e6-a1-86-e6-9e-b6-e5-ad-a6-e4-b9-a0-e7-ac-94-e8-ae-b0-e4-ba-8c%2F</url>
    <content type="text"><![CDATA[期货交易App 首先，我们的程序后台作为一个client与CTP（期货行情源）也就是server建立长连接，以事件驱动的方式对接收到的实时行情数据进行持久化（行情数据：买卖价、开收价、持有量等）和业务处理（涉及交易、风控等）。同时地，持久化的数据将在用户使用app（每个用户可以看作一个client）时，被我们持续分发到每个连接通道中，最终被app客户端用作视图渲染，下单操作等。 客户端启动代码：1234567891011121314151617181920212223242526public class NettyClient&#123;public static void connect(NettyHandler handler, int workers, String host, int port) throws Exception &#123; ExecutorService pool= Executors.newFixedThreadPool(Math.max(workers, 1)); EventLoopGroup workerGroup = new NioEventLoopGroup(1); try &#123; Bootstrap b = new Bootstrap(); b.group(workerGroup).channel(NioSocketChannel.class).handler(new InitHandler(handler, pool)); b.connect(host, port).sync().channel().closeFuture().sync();&#125; finally &#123; workerGroup.shutdownGracefully(); pool.shutdown();&#125;&#125;public static void main(String[] args) throws Exception &#123;new Thread() &#123; public void run() &#123; try &#123; NettyClient.connect(new MyHandler2(), 1, &quot;127.0.0.1&quot;, 33333); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;.start();&#125;&#125; 服务端启动代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class NettyServer &#123; public static void start(NettyHandler handler, int workers, int port) throws Exception &#123; ExecutorService pool=Executors.newFixedThreadPool(Math.max(workers, 1)); EventLoopGroup bossGroup = new NioEventLoopGroup(1); // 用于accept incomming连接请求 EventLoopGroup workerGroup = new NioEventLoopGroup();// 程序将accepted连接请求注册到workerGroup中，并由里面的线程跟踪处理相关“traffic” try &#123; ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup).channel(NioServerSocketChannel.class).childHandler(new InitHandler(handler,pool)); // 利用 NioServerSocketChannel class 的初始化Server端的Channel实例来accept连接请求 // 设置另外一个handler，childHanlder与handler的不同会在后续博文中描述，Handler在Netty是一个很重要的地位，相当于Servlet中的filter一样，这里用一个ChannelInitializer来统一管理所有handler b.bind(port).sync().channel().closeFuture().sync(); // 绑定端口，并利用sync()方法开始监听端口（等待客户端连接请求），若成功，返回ChannelFuture，ChannelFuture也会在后续详细说明 // 监听服务关闭监听 &#125; finally &#123; bossGroup.shutdownGracefully();// 相当于是关闭线程池 workerGroup.shutdownGracefully(); &#125; &#125;&#125;public static void main(String[] args) throws Exception &#123; try &#123; NettyServer.start(new NettyHandler() &#123; AtomicInteger ai=new AtomicInteger(); @Override public void handleMsg(final Channel channel, String json) &#123; //System.out.println(ai.incrementAndGet()+&quot; &quot;+System.currentTimeMillis()+&quot; &quot;+json); System.out.println(&quot;服务端向客户端 输入数据&quot;);&lt;/p&gt; Timer time = new Timer(); time.schedule(new TimerTask() &#123; @Override public void run() &#123; channel.writeAndFlush(&quot;接受数据&quot;+ai.incrementAndGet()); &#125; &#125;,1000l,5000l); &#125; @Override public void channelRemoved(Channel channel) &#123; System.out.println(&quot;remove&quot;); &#125; @Override public void channelRegistered(Channel channel) &#123; channel.writeAndFlush(&quot;初次握手&quot;); System.out.println(&quot;有新的客户端连入&quot;); &#125; &#125;, 16, 33333); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125;&#125; NettyHanlder:业务处理核心部分，可监听通道建立、端口以及数据接受事件12345public interface NettyHandler &#123; public void channelRegistered(io.netty.channel.Channel channel); public void handleMsg(io.netty.channel.Channel channel, String json); public void channelRemoved(io.netty.channel.Channel channel);&#125; InitHandler:通过封装使得Client与Server可以共用一个ChannelInitializer12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&lt;br&gt;&lt;/br&gt;/** * 描述:ChannelInitializer的主要目的是为SocketChannel注册Handler的主要目的是为SocketChannel注册Handler， * 也就是当一个SocketChannel被Server accept 进来以后，进来的数据要分别按注册先后顺序执行哪些handler， * 出去的数据要分别按逆序的顺序执行哪些handler * * @author 陈润发 * @created 16/9/3 * @since v1.0.0 */@Componentpublic class InitHandler extends ChannelInitializer&lt;SocketChannel&gt; &#123; private final static int readerIdleTimeSeconds = 40;//读操作空闲30秒 private final static int writerIdleTimeSeconds = 50;//写操作空闲60秒 private final static int allIdleTimeSeconds = 100;//读写全部空闲100秒 NettyHandler handler; ExecutorService pool; public InitHandler(NettyHandler handler,ExecutorService worker)&#123; this.handler=handler; pool=worker; &#125; private SimpleChannelInboundHandler&lt;String&gt; coreHandler=new SimpleChannelInboundHandler&lt;String&gt;()&#123; public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; handler.channelRegistered(ctx.channel()); &#125;; @Override protected void channelRead0(final ChannelHandlerContext ctx, final String msg) throws Exception &#123; pool.execute(new Runnable() &#123; @Override public void run() &#123; handler.handleMsg(ctx.channel(), msg); &#125; &#125;); &#125; public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; handler.channelRemoved(ctx.channel()); &#125;; public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; ctx.close(); cause.printStackTrace(); handler.channelRemoved(ctx.channel()); &#125;; public boolean isSharable() &#123; return true; &#125; &#125;; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); //ChannelInboundHandler都是按顺序执行的，ChanneloutboundHandler都是按逆序： pipeline.addLast(new LuckinDecoder()); //继承至ChannelInboundHandlerAdapter pipeline.addLast(new LuckinEncoder()); //继承至ChannelOutboundHandlerAdapter pipeline.addLast(coreHandler); //继承至ChannelInboundHandlerAdapter &#125;&#125; LuckinEecoder：数据解码，同时可做数据舍弃，如接受到的数据小于4个字节认为无效数据12345678910111213141516171819202122232425&lt;br&gt;&lt;/br&gt;public class LuckinDecoder extends ByteToMessageDecoder &#123; @Override protected void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception &#123; while (true) &#123; if (in.readableBytes() &lt;= 4) &#123; break; &#125; in.markReaderIndex(); int length = in.readInt(); if(length&lt;=0)&#123; throw new Exception(&quot;a negative length occurd while decode!&quot;); &#125; if (in.readableBytes() &lt; length) &#123; in.resetReaderIndex(); break; &#125; byte[] msg = new byte[length]; in.readBytes(msg); out.add(new String(msg, &quot;GBK&quot;)); &#125; &#125;&#125; LuckinEncoder:对数据编码1234567891011121314public class LuckinEncoder extends MessageToByteEncoder&lt;String&gt; &#123; @Override protected void encode(ChannelHandlerContext ctx, String msg, ByteBuf out) throws Exception &#123;// System.out.println(&quot;begin encode&quot;); if(StringUtils.isEmpty(msg))&#123; return ; &#125; byte[] message=msg.getBytes(&quot;GBK&quot;); out.writeInt(message.length); out.writeBytes(message);// System.out.println(&quot;end encode&quot;); &#125;&#125; 分析 对于Server，适配的结果：LuckinEecoder -&gt; (CTP接受后进行Decoder) 对于Client，适配的结果：LuckinDecoder -&gt; CoreHandler - - - - - -]]></content>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty通信框架学习笔记一]]></title>
    <url>%2F2016%2F11%2F30%2Fnetty-e9-80-9a-e4-bf-a1-e6-a1-86-e6-9e-b6-e5-ad-a6-e4-b9-a0-e7-ac-94-e8-ae-b0-e4-b8-80%2F</url>
    <content type="text"><![CDATA[引用官方的介绍：Netty是一款异步以事件为驱动的网络开发框架和工具，能够快速的帮助开发者开发出可维护的高性能，高扩张性的服务器和客户端。换句话说，Netty是一款可以快速和方便的开发出服务器和客户端应用的NIO框架。它使得开发TCP, UDP服务应用变得非常方便。快速和简单并不会影响应用的性能和可维护性。Netty采用的精心的设计并且实现很多协议例如FTP, SMTP, HTTP 各种二进制数据和文本协议。结果,Netty在性能，稳定性，灵活性上都做得非常好。 网上：Netty是一套在java NIO的基础上封装的便于用户开发网络应用程序的api. 应用场景很多,诸如阿里的消息队列(RocketMQ),分布式rpc(Dubbo)通信层都使用到了netty(dubbo可以用服务发现自由选择通信层)。 到XHB已经四个多月了，之前一直在YQB组做期货交易软件相关的开发，用到了Netty（4.0）这个网络通信框架，一直想找机会整理这部分相关的笔记，于是有了开篇。 Netty中的组件 Boss:启动一个server实例只会产生一个boss线程，boss线程主要负责监听端口。 Worker:当有新的连接请求时boss就会产生一个task交给worker线程池处 理，worker线程池中线程的个数默认是cpu个数的2倍,不负责处理业务逻辑，只是起到分发请求到handler的作用。 ChannelPipelineFactory：产生ChannelPipe的工厂类。 Channel:负责数据读，写的对象，有点类似于老的io里面的stream。它和stream的区别，channel是双向的，既可以write 也可以read，而stream要分outstream和inputstream。而且在NIO中用户不应该直接从channel中读写数据，而是应该通过buffer，通过buffer再将数据读写到channel中。同事提供了许多通道信息，如：状态、ChannelConfig。 ChannelEvent:是数据或者状态的载体，若是以server为主体的话，从client的数据到server的过程是Upstream；而server到client的数据传输过程叫downstream；而如果以client为主体的话，从server到client的过程对client来说是Upstream，而client到server的过程对client来说就是downstream。 Pipeline:负责维护所有的Handler。 ChannelPipeline:ChannelHandler的容器,一个Channel包含一个ChannelPipeline，所有ChannelHandler都会注册到ChannelPipeline中，并按顺序组织起来。 ChannelContext:一个Channel一个，是Handler和Pipeline的中间件。 ChannelHandler:对Channel事件的处理器，可分两类ChannelInboundHandler和ChannelOutboundHandler。 Sink:负责和底层的交互如bind，Write，Close等。 几个图示ChannelPipeline线程模型]]></content>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven通过profile构建不同环境配置包]]></title>
    <url>%2F2016%2F11%2F24%2Fmaven-e9-80-9a-e8-bf-87profile-e6-9e-84-e5-bb-ba-e4-b8-8d-e5-90-8c-e7-8e-af-e5-a2-83-e9-85-8d-e7-bd-ae-e5-8c-85%2F</url>
    <content type="text"><![CDATA[一个项目里总会有很多配置文件。而且一般都会有多套环境。开发的、测试的、预上线、正式的。而在这些环境的配置都不太一样。比如数据库、redis等，这些都是很常见的。所以在打包的时候就需要根据不同的环境打包。 下面讲的就是利用Maven为不同环境建立不同的环境包，下面是我的目录结构图: 在一个标准的maven项目rescourse目录下添加了一个config目录，config目录中有develop和product等多个个目录。分别代表开发和生产环境（当然你可以自行添加更多配置目录） 然后在pom.xml中添加下面的xml： 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;profiles&gt; &lt;!-- 默认激活 dev 开发环境 --&gt; &lt;!-- product使用 mvn xxx -P product --&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;develop&lt;/id&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;/activation&gt; &lt;properties&gt; &lt;cfg_dir&gt;develop&lt;/cfg_dir&gt; &lt;/properties&gt; &lt;build&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;$&#123;basedir&#125;/src/main/resources&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;!-- 排除resource下所有文件--&gt; &lt;excludes&gt; &lt;exclude&gt;**&lt;/exclude&gt; &lt;/excludes&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;!-- 保留指定路径下的指定后缀文件--&gt; &lt;includes&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/build&gt; &lt;/profile&gt; &lt;profile&gt; &lt;id&gt;product&lt;/id&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;/activation&gt; &lt;properties&gt; &lt;cfg_dir&gt;product&lt;/cfg_dir&gt; &lt;/properties&gt; &lt;/profile&gt;&lt;/profiles&gt; 有时候我们希望打包的时候指定为jar包以减小空间，就应该将这些区分用途的配置信息过滤掉，只保留必要的如*.xml和.properties文件， 则可以通过resource标签达到我们的需求。 注意：如果是开源的项目，那么应该将product的所以相关文件通过.gitignore文件进行了排除，这样避免私密配置暴露。]]></content>
      <tags>
        <tag>构建工具</tag>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nxlog安装及使用]]></title>
    <url>%2F2016%2F11%2F21%2Fnxlog-e5-ae-89-e8-a3-85-e5-8f-8a-e4-bd-bf-e7-94-a8%2F</url>
    <content type="text"><![CDATA[安装Nxlog121.wget http://nxlog.org/system/files/products/files/1/nxlog-ce-2.9.1716-1_rhel7.x86_64.rpm2.yum localinstall nxlog-ce-2.9.1716-1_rhel7.x86_64.rpm -y 配置nxlog.conf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354## This is a sample configuration file. See the nxlog reference manual about the## configuration options. It should be installed locally under## /usr/share/doc/nxlog-ce/ and is also available online at## http://nxlog.org/docs######################################### Global directives #########################################User nxlogGroup nxlogLogFile /var/log/nxlog/nxlog.logLogLevel INFO######################################### Modules ##########################################&lt;Extension _syslog&gt; # Module xm_syslog#&lt;/Extension&gt;&lt;Input in1&gt; Module im_file File &quot;/data/server/logs/admin/admin.log&quot; SavePos TRUE&lt;/Input&gt;&lt;Input in2&gt; Module im_file File &quot;/data/stock_server/log/stock_futuresquota/futuresquota.log&quot; SavePos TRUE&lt;/Input&gt;&lt;Output out&gt; Module om_udp Host xxx.xxx.xxx(graylog) Port 12601（需要与graylog约定的输入源配置一致）&lt;/Output&gt;######################################### Routes #########################################&lt;Route 1&gt; Path in1 =&gt; out&lt;/Route&gt;&lt;Route 2&gt; Path in2 =&gt; out&lt;/Route&gt;~~~ 运行1service nxlog start]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>日志收集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 6.8和7.x Docker的安装方式]]></title>
    <url>%2F2016%2F11%2F12%2Fcentos-6-8-e5-92-8c7-x-docker-e7-9a-84-e5-ae-89-e8-a3-85-e6-96-b9-e5-bc-8f%2F</url>
    <content type="text"><![CDATA[在CentOS6.8下安装Docker1234[root@bogon yum.repos.d]# uname -aLinux bogon 2.6.32-642.el6.x86_64 #1 SMP Tue May 10 17:27:01 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux[root@bogon yum.repos.d]# cat /etc/redhat-release CentOS release 6.8 (Final) 安装EPEL 因为系统自带的repo中不带docker需要安装epel rpm -Uvh http://ftp.riken.jp/Linux/fedora/epel/6Server/x86_64/epel-release-6-8.noarch.rpm 安装Docker 1yum install -y docker-io 开机自启动与启动Docker 1234567[root@bogon yum.repos.d]# service docker startStarting cgconfig service: [ OK ]Starting docker: [ OK ][root@bogon yum.repos.d]# chkconfig docker on[root@bogon yum.repos.d]# chkconfig docker --listdocker 0:off 1:off 2:on 3:on 4:on 5:on 6:off[root@bogon yum.repos.d]# CentOS 7.x 系列安装 Docker 系统要求：64 位操作系统，内核版本至少为 3.10。 Docker 目前支持 CentOS 6.5 及以后的版本，推荐使用 CentOS 7 系统。 首先，添加 yum 软件源。 12345678$ sudo tee /etc/yum.repos.d/docker.repo &lt;&lt;-&apos;EOF&apos;[dockerrepo]name=Docker Repositorybaseurl=https://yum.dockerproject.org/repo/main/centos/$releasever/enabled=1gpgcheck=1gpgkey=https://yum.dockerproject.org/gpgEOF 接着更新 yum 软件源缓存，并安装 docker-engine。12$ sudo yum update$ sudo yum install -y docker-engine 另外，也可以使用官方提供的脚本来安装 Docker。1$ sudo curl -sSL https://get.docker.com/ | sh 最后配置让 Docker 服务在系统启动后自动启动。1$ sudo chkconfig docker on]]></content>
      <tags>
        <tag>Docker</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jenkins在Mac上的服务启动&关闭]]></title>
    <url>%2F2016%2F11%2F12%2Fjenkins-e5-9c-a8mac-e4-b8-8a-e7-9a-84-e6-9c-8d-e5-8a-a1-e5-90-af-e5-8a-a8-e5-85-b3-e9-97-ad%2F</url>
    <content type="text"><![CDATA[#启动sudo launchctl load /Library/LaunchDaemons/org.jenkins-ci.plist #停止 sudo launchctl unload /Library/LaunchDaemons/org.jenkins-ci.plist]]></content>
      <tags>
        <tag>Jenkins</tag>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[timer.schedule()运行一段时间自动停止的可能原因]]></title>
    <url>%2F2016%2F10%2F21%2Ftimer-schedule-e8-bf-90-e8-a1-8c-e4-b8-80-e6-ae-b5-e6-97-b6-e9-97-b4-e8-87-aa-e5-8a-a8-e5-81-9c-e6-ad-a2-e7-9a-84-e5-8f-af-e8-83-bd-e5-8e-9f-e5-9b-a0%2F</url>
    <content type="text"><![CDATA[场景： 在一个定时将mongo里的k线数据更新到内存变量的task中，可能会因为run()方法内的某些异常未进行捕获导致task停止 解决方案： 将可能产生异常的代码try catch一下; 使用ScheduledThreadPoolExecutor代替，如果有异常，用ScheduledThreadPoolExecutor还是不会继续执行的，只不过因为线程池的存在，程序不会结束。Timer和ScheduleExecutorService的区别在于一个单线程一个多线程。]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jenkins基于docker的安装方式]]></title>
    <url>%2F2016%2F09%2F12%2Fjenkins-e5-ae-89-e8-a3-85-e6-96-b9-e5-bc-8f-e4-bb-8b-e7-bb-8d%2F</url>
    <content type="text"><![CDATA[Jenkins安装的方式大概分为三种： 通过Tomcat运行，将官网下载的最新版jenkins WAR文件直接放到Tomcat的webaps目录运行即可。 在Mac上进行可以直接下载pkg文件安装。前两种方式均通过访问http://localhost:8080进入Jenkins的可视化管理界面。ps:境外网站的原因下载缓慢，建议以wget的方式下载，保证文件完整性。 Docker的方式：将jenkins运行在容器当中 。（推荐） Docker方式的安装过程： 通过官方镜像源拉取镜像：https://hub.docker.com/_/jenkins/，由于拉取速度实在太慢的原因，不推荐。 1docker run --name myjenkins -d -p 8080:8080 -v /var/jenkins_home jenkins 参数不详细解释，详情见 docker常用命令及入门教程 相对较快的另一种方法，采取通过官方jenkins源码构建镜像的方式,首先拉取源码： 1git clone https://github.com/jenkinsci/docker.git 然后build+Dockerfile构建jenkins镜像 1docker build -t xhuabu/jenkins . 镜像构建成功后（总共21个Step，Dockerfile共21条指令），使用docker images查看镜像， 1docker images 后台跑起容器，同时指定日志配置文件及挂载目录： 123456mkdir datacat &gt; data/log.properties &lt;&lt;EOFhandlers=java.util.logging.ConsoleHandlerjenkins.level=FINESTjava.util.logging.ConsoleHandler.level=FINESTEOF 1docker run --name myjenkins -d -p 8080:8080 -p 50000:50000 --env JAVA_OPTS=&quot;-Djava.util.logging.config.file=/var/jenkins_home/log.properties&quot; -v `pwd`/data:/var/jenkins_home jenkins]]></content>
  </entry>
  <entry>
    <title><![CDATA[Jenkins基于Yum的安装及Maven安装]]></title>
    <url>%2F2016%2F09%2F11%2Fjenkins-e5-9f-ba-e4-ba-8eyum-e7-9a-84-e5-ae-89-e8-a3-85-e5-8f-8amaven-e5-ae-89-e8-a3-85%2F</url>
    <content type="text"><![CDATA[通过Yum的方式安装Jenkins 创建 jenkins 的 yum 的源文件 1$ vim /etc/yum.repos.d/jenkins.repo 在 jenkins.repo 中加入以下内容： 1234[jenkins]name=Jenkinsbaseurl=http://pkg.jenkins-ci.org/redhatgpgcheck=1 rpm 增加 jenkins 源的 key 1$ sudo rpm --import http://pkg.jenkins-ci.org/redhat/jenkins-ci.org.key 安装 jenkins 1$ sudo yum install jenkins 安装成功 - 接下来修改Jenkins配置文件：主要可以配置 jenkins 的运行端口和监听端口，以及启动用户，配置文件地址为 /etc/sysconfig/jenkins ，下面为主要修改内容： 12345$ sudo vim /etc/sysconfig/jenkins# 修改启动用户为root,默认为jenkinsJENKINS_USER=&quot;root&quot;# 修改运行端口为10086，默认为8080JENKINS_PORT=&quot;10086&quot; 启动 jenkins服务（start/stop/restart），并设置为开机自启动 12sudo service jenkins startsudo chkconfig jenkins on 修改防火墙配置或者先关闭防火墙 1234567firewall-cmd --zone=public --add-port=10086/tcp --permanentfirewall-cmd --zone=public --add-service=http --permanentfirewall-cmd --reloadsystemctl start firewalld.service#启动firewallsystemctl stop firewalld.service#停止firewallsystemctl disable firewalld.service#禁止firewall开机启动 访问先前设置的端口所在页面，见如下图： - 前往/var/lib/jenkins/secrets/initialAdminPassword 复制密码粘贴后运行下一步，按照提示安装插件即可。 - 安装完毕。 Maven下载与安装 在maven官网下载安装包：http://maven.apache.org/download.cgi 可以利用scp或者其他ftp工具，将文件拷贝至/usr/local，然后解压 1tar -zvxf apache-maven-3.3.9-bin.tar.gz 设置软连接，方便日后管理 1ln -s apache-maven-3.3.9 apache-maven 配置环境变量，编辑/etc/profile，插入以下环境变量 12export MAVEN_HOME=/usr/local/apache-mavenexport PATH=$PATH:$MAVEN_HOME/bin 让其生效 1source /etc/profile 验证是否成功安装（能看到版本信息一般即视为安装成功） 1mvn -version]]></content>
      <tags>
        <tag>Jenkins</tag>
        <tag>持续集成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitFlow工作流介绍和使用]]></title>
    <url>%2F2016%2F09%2F10%2Fgitflow--e6-bc-94-e7-a4-ba-e5-b1-81-e5-b1-81-e8-b8-a2%2F</url>
    <content type="text"><![CDATA[Git Flow入门与实践 Git Flow工作流定义了一个围绕项目发布的严格分支模型,它为管理更大规模的项目提供了健壮的框架。 流程图 - - - - - - 流程图 特点 首先，项目有两个长期分支: 123- 主分支 master (对外发布版本)- 开发分支 develop （最新开发版本） 其次，项目存在三种短期分支: 12345- 功能分支（feature branch）- 补丁分支（hotfix branch）- 预发分支（release branch） 一旦完成开发，它们就会被合并进develop或master，然后被删除。 - - - - - - 长期分支 （master/develop） 相对使用仅有的一个master分支，Gitflow工作流使用2个分支来记录项目的历史。master分支上存放的是随时可供在生产环境中部署的代码，而develop分支是保存当前最新开发成果的分支。 - - - - - - 功能分支 （feature/*） 项目中所有新开发的功能都从develop分支出来，可以推送到远端以作备份和协作。当新功能完成时，再合并回develop分支。 一般而言，feature分支代码可以保存在开发者自己的代码库中而不强制提交到主代码库里。 - - - - - - 预发分支 ( release/* ) release分支是为发布新的产品版本而设计的。在这个分支上的代码允许做小的缺陷修正、准备发布版本所需的各项说明信息（版本号、发布时间、编译时间等等）。通过在release分支上进行这些工作可以让develop分支空闲出来以接受新的feature分支上的代码提交，进入新的软件开发迭代周期。(不能再将新的代码合并到release上) 发布Release分支时，合并Release到Master和Develop， 同时在Master分支上打个Tag记住Release版本号，然后可以删除Release分支了。 发布版本 补丁分支(hotfix/*） 当生产环境中的软件遇到了异常情况或者发现了严重到必须立即修复的软件缺陷的时候，就需要从master分支上指定的TAG版本派生hotfix分支来组织代码的紧急修复工作。 这样做的显而易见的好处是不会打断正在进行的develop分支的开发工作，能够让团队中负责新功能开发的人与负责代码紧急修复的人并行的开展工作 修复紧急问题 Git Flow代码示例a. 创建develop分支 12git branch developgit push -u origin develop b. 开始新Feature开发 12345678git checkout -b feature/some-feature develop# Optionally, push branch to origin:git push -u origin some-feature # 做一些改动 git statusgit add some-filegit commit c. 完成Feature 12345678git pull origin developgit checkout developgit merge --no-ff feature/some-featuregit push origin developgit branch -d feature/some-feature# If you pushed branch to origin:git push origin --delete feature/some-feature d. 开始Relase 12git checkout -b release/rev-0.1.0 develop# Optional: Bump version number, commit # Prepare release, commit e. 完成Release 123456789101112131415git checkout mastergit merge --no-ff release/rev-0.1.0git pushgit checkout developgit merge --no-ff release/rev-0.1.0git pushgit branch -d release/rev-0.1.0# If you pushed branch to origin:git push origin --delete release/rev-0.1.0 git tag -a v0.1.0 master git push –tags f. 开始Hotfix 1git checkout -b hotfix/fix-0.1.1 master g. 完成Hotfix 12345678910111213git checkout mastergit merge --no-ff hotfix/fix-0.1.1git pushgit checkout developgit merge --no-ff hotfix/fix-0.1.1git pushgit branch -d hotfix/fix-0.1.1git tag -a v0.1.1 mastergit push --tags Git Flow工具 SourceTree 功能界面 参考 Git 在团队中的最佳实践–如何正确使用Git Flow Git 工作流程 Git工作流指南：Gitflow工作流]]></content>
      <tags>
        <tag>Git</tag>
        <tag>效率工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日志收集平台Graylog2以及Nxlog的安装与配置（基于docker）]]></title>
    <url>%2F2016%2F09%2F03%2F-e6-97-a5-e5-bf-97-e6-94-b6-e9-9b-86-e5-b9-b3-e5-8f-b0graylog2-e4-bb-a5-e5-8f-8anxlog-e7-9a-84-e5-ae-89-e8-a3-85-e4-b8-8e-e9-85-8d-e7-bd-ae-ef-bc-88-e%2F</url>
    <content type="text"><![CDATA[graylog 安装配置 CentOS7 64 基于Docker的安装 graylog 官网 docker 安装 Persist log data 1234mkdir -p /graylog/configcd /graylog/configwget https://raw.githubusercontent.com/Graylog2/graylog2-images/2.0/docker/config/graylog.confwget https://raw.githubusercontent.com/Graylog2/graylog2-images/2.0/docker/config/log4j2.xml 通过docker安装graylog121.挂载- /etc/localtime:/etc/localtime 容器同步宿主机时间 2.修改GRAYLOG_REST_TRANSPORT_URI 地址为宿主机IP The docker-compose.yml file looks like this:123456789101112131415161718192021222324252627282930some-mongo: image: &quot;mongo:3&quot; volumes:- /graylog/data/mongo:/data/db- /etc/localtime:/etc/localtimesome-elasticsearch: image: &quot;elasticsearch:2&quot; command: &quot;elasticsearch -Des.cluster.name=&apos;graylog&apos;&quot; volumes:- /graylog/data/elasticsearch:/usr/share/elasticsearch/data- /etc/localtime:/etc/localtimegraylog: image: graylog2/server volumes:- /graylog/data/journal:/usr/share/graylog/data/journal- /graylog/config:/usr/share/graylog/data/config- /etc/localtime:/etc/localtime environment:GRAYLOG_PASSWORD_SECRET: somepasswordpepperGRAYLOG_ROOT_PASSWORD_SHA2: 8c6976e5b5410415bde908bd4dee15dfb167a9c873fc4bb8a81f6f2ab448a918GRAYLOG_REST_TRANSPORT_URI: http://127.0.0.1:12900 links:- some-mongo:mongo- some-elasticsearch:elasticsearch ports:- &quot;9000:9000&quot;- &quot;12900:12900&quot;- &quot;12201/udp:12201/udp&quot;- &quot;1514/udp:1514/udp&quot; 1.docker-compose安装环境,安装pip1install pip 2.安装docker-comopse1pip install -U docker-compose 3.启动docker-compose1docker-compose up Docker Compose常用命令 Graylog Collector Sidecar 原理图 安装nxlog 1.centos 环境 安装方法参考这里http://blog.csdn.net/iwannarun/article/details/52604646 2.ubuntu 环境12345$ sudo dpkg-deb -f nxlog-ce_2.9.1716_ubuntu_1604_amd64.deb Depends$ sudo apt-get -f -y install$ sudo /etc/init.d/nxlog stop$ sudo update-rc.d -f nxlog remove$ sudo gpasswd -a nxlog adm 3.Windows 安装collector-sidecar1.centos 环境2.ubuntu 环境 dpkg -i collector-sidecar_0.0.7-1_amd64.deb123To register and enable a system service use the -service option:$ sudo graylog-collector-sidecar -service install$ sudo service collector-sidecar start 3.Windows nxlog.conf、collector_sidecar.yml 配置 1.nxlog.conf 1234567891011路径为要监控日志文件路径graylog server ip 端口可以不用修改 2.collector_sidecar.yml ![这里写图片描述](http://img.blog.csdn.net/20161013000529424)还要修改server_url ：http://graylog server ip:12900node_id: graylog-collector-sidecar 如果有多个collect则要修改不同名称graylog web 配置 http://graylog server ip:9000system/collectors-collectors-manager coonfigures-create coonfigures``` Collector xx Configuration - NXLog 配置 Configure NXLog Outputs Configure NXLog Inputs 日志客户端启动collector-sidecar /etc/init.d/collector-sidecar start 查看进程 graylog web查看Collectors 是否运行正常 graylog web 设置 日志接收 收集成果展示 DashBoards：可根据检索条件设置饼状图柱状图，便于分析查看 Search : 可根据时间关键字及日志源（Nxlog）所在的服务器进行筛选 总结graylog非常适合分布式应用的日志收集，将各台服务器的日志收集至同一个地方，利用预设检索场景的dashboards能够对全局达到一目了然的效果，检索信息也十分方便。但需要注意的是日志规范在这里就尤为重要：例如统一的日志的格式能够提升检索速度，生成环境只坚决不打debug级别的日志能有效控制空间占用，毕竟接受的日志数据是持久化在graylog的mongo当中的。]]></content>
      <tags>
        <tag>日志收集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx配置文件详解]]></title>
    <url>%2F2016%2F08%2F16%2Fnginx-e9-85-8d-e7-bd-ae-e6-96-87-e4-bb-b6-e8-af-a6-e8-a7-a3%2F</url>
    <content type="text"><![CDATA[Nginx配置文件主要分为六个区域： main(全局设置)、events(nginx工作模式)、http(http设置)、 sever(主机设置)、location(URL匹配)、upstream(负载均衡服务器设置)。1234567891011121314151617181920212223mainevents &#123; ....&#125;http &#123; .... upstream myproject &#123; ..... &#125; server &#123; .... location &#123; .... &#125; &#125; server &#123; .... location &#123; .... &#125; &#125; ....&#125; 具体实例：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146nginx.conf#定义Nginx运行的用户和用户组user www www;#nginx进程数，建议设置为等于CPU总核心数。worker_processes 8;#全局错误日志定义类型，[ debug | info | notice | warn | error | crit ]error_log /var/log/nginx/error.log info;#进程文件pid /var/run/nginx.pid;#一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（系统的值ulimit -n）与nginx进程数相除，但是nginx分配请求并不均匀，所以建议与ulimit -n的值保持一致。worker_rlimit_nofile 65535;#工作模式与连接数上限events&#123;#参考事件模型，use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; epoll模型是Linux 2.6以上版本内核中的高性能网络I/O模型，如果跑在FreeBSD上面，就用kqueue模型。use epoll;#单个进程最大连接数（最大连接数=连接数*进程数）worker_connections 65535;&#125;#设定http服务器http&#123;include mime.types; #文件扩展名与文件类型映射表default_type application/octet-stream; #默认文件类型#charset utf-8; #默认编码server_names_hash_bucket_size 128; #服务器名字的hash表大小client_header_buffer_size 32k; #上传文件大小限制large_client_header_buffers 4 64k; #设定请求缓client_max_body_size 8m; #设定请求缓sendfile on; #开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成off。autoindex on; #开启目录列表访问，合适下载服务器，默认关闭。tcp_nopush on; #防止网络阻塞tcp_nodelay on; #防止网络阻塞keepalive_timeout 120; #长连接超时时间，单位是秒#FastCGI相关参数是为了改善网站的性能：减少资源占用，提高访问速度。下面参数看字面意思都能理解。fastcgi_connect_timeout 300;fastcgi_send_timeout 300;fastcgi_read_timeout 300;fastcgi_buffer_size 64k;fastcgi_buffers 4 64k;fastcgi_busy_buffers_size 128k;fastcgi_temp_file_write_size 128k;#gzip模块设置gzip on; #开启gzip压缩输出gzip_min_length 1k; #最小压缩文件大小gzip_buffers 4 16k; #压缩缓冲区gzip_http_version 1.0; #压缩版本（默认1.1，前端如果是squid2.5请使用1.0）gzip_comp_level 2; #压缩等级gzip_types text/plain application/x-javascript text/css application/xml;#压缩类型，默认就已经包含text/html，所以下面就不用再写了，写上去也不会有问题，但是会有一个warn。gzip_vary on;#limit_zone crawler $binary_remote_addr 10m; #开启限制IP连接数的时候需要使用upstream blog.ha97.com &#123;#upstream的负载均衡，weight是权重，可以根据机器配置定义权重。weigth参数表示权值，权值越高被分配到的几率越大。server 192.168.80.121:80 weight=3;server 192.168.80.122:80 weight=2;server 192.168.80.123:80 weight=3;&#125;#虚拟主机的配置server&#123;#监听端口listen 80;#域名可以有多个，用空格隔开server_name www.ha97.com ha97.com;index index.html index.htm index.php;root /data/www/ha97;location ~ .*\.(php|php5)?$&#123;fastcgi_pass 127.0.0.1:9000;fastcgi_index index.php;include fastcgi.conf;&#125;#图片缓存时间设置location ~ .*\.(gif|jpg|jpeg|png|bmp|swf)$&#123;expires 10d;&#125;#JS和CSS缓存时间设置location ~ .*\.(js|css)?$&#123;expires 1h;&#125;#日志格式设定log_format access &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos;&apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos;&apos;&quot;$http_user_agent&quot; $http_x_forwarded_for&apos;;#定义本虚拟主机的访问日志access_log /var/log/nginx/ha97access.log access;#对 &quot;/&quot; 启用反向代理location / &#123;proxy_pass http://127.0.0.1:88;proxy_redirect off;proxy_set_header X-Real-IP $remote_addr;#后端的Web服务器可以通过X-Forwarded-For获取用户真实IPproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;#以下是一些反向代理的配置，可选。proxy_set_header Host $host;client_max_body_size 10m; #允许客户端请求的最大单文件字节数client_body_buffer_size 128k; #缓冲区代理缓冲用户端请求的最大字节数，proxy_connect_timeout 90; #nginx跟后端服务器连接超时时间(代理连接超时)proxy_send_timeout 90; #后端服务器数据回传时间(代理发送超时)proxy_read_timeout 90; #连接成功后，后端服务器响应时间(代理接收超时)proxy_buffer_size 4k; #设置代理服务器（nginx）保存用户头信息的缓冲区大小proxy_buffers 4 32k; #proxy_buffers缓冲区，网页平均在32k以下的设置proxy_busy_buffers_size 64k; #高负荷下缓冲大小（proxy_buffers*2）proxy_temp_file_write_size 64k;#设定缓存文件夹大小，大于这个值，将从upstream服务器传&#125;#设定查看Nginx状态的地址location /NginxStatus &#123;stub_status on;access_log on;auth_basic &quot;NginxStatus&quot;;auth_basic_user_file conf/htpasswd;#htpasswd文件的内容可以用apache提供的htpasswd工具来产生。&#125;#本地动静分离反向代理配置#所有jsp的页面均交由tomcat或resin处理location ~ .(jsp|jspx|do)?$ &#123;proxy_set_header Host $host;proxy_set_header X-Real-IP $remote_addr;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;proxy_pass http://127.0.0.1:8080;&#125;#所有静态文件由nginx直接读取不经过tomcat或resinlocation ~ .*.(htm|html|gif|jpg|jpeg|png|bmp|swf|ioc|rar|zip|txt|flv|mid|doc|ppt|pdf|xls|mp3|wma)$&#123; expires 15d; &#125;location ~ .*.(js|css)?$&#123; expires 1h; &#125;&#125;&#125;]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx反向代理与负载均衡]]></title>
    <url>%2F2016%2F08%2F16%2Fnginx-e5-8f-8d-e5-93-8d-e4-bb-a3-e7-90-86-e4-b8-8e-e8-b4-9f-e8-bd-bd-e5-9d-87-e8-a1-a1%2F</url>
    <content type="text"><![CDATA[Nginx的反向代理 在说啥啥反向代理之前，先说下什么是代理或者正向代理。 正向代理 A同学在大众创业、万众创新的大时代背景下开启他的创业之路，目前他遇到的最大的一个问题就是启动资金，于是他决定去找马云爸爸借钱，可想而知，最后碰一鼻子灰回来了，情急之下，他想到一个办法，找关系开后门，经过一番消息打探，原来A同学的大学老师王老师是马云的同学，于是A同学找到王老师，托王老师帮忙去马云那借500万过来，当然最后事成了。不过马云并不知道这钱是A同学借的，马云是借给王老师的，最后由王老师转交给A同学。这里的王老师在这个过程中扮演了一个非常关键的角色，就是代理，也可以说是正向代理，王老师代替A同学办这件事，这个过程中，真正借钱的人是谁，马云是不知道的，这点非常关键。 我们常说的代理也就是只正向代理，正向代理的过程，它隐藏了真实的请求客户端，服务端不知道真实的客户端是谁，客户端请求的服务都被代理服务器代替来请求，某些科学上网工具扮演的就是典型的正向代理角色。用浏览器访问 http://www.google.com 时，被残忍的block，于是你可以在国外搭建一台代理服务器，让代理帮我去请求google.com，代理把请求返回的相应结构再返回给我。 反向代理 大家都有过这样的经历，拨打10086客服电话，可能一个地区的10086客服有几个或者几十个，你永远都不需要关心在电话那头的是哪一个，叫什么，男的，还是女的，漂亮的还是帅气的，你都不关心，你关心的是你的问题能不能得到专业的解答，你只需要拨通了10086的总机号码，电话那头总会有人会回答你，只是有时慢有时快而已。那么这里的10086总机号码就是我们说的反向代理。客户不知道真正提供服务人的是谁。 反向代理隐藏了真实的服务端，当我们请求 www.baidu.com 的时候，就像拨打10086一样，背后可能有成千上万台服务器为我们服务，但具体是哪一台，你不知道，也不需要知道，你只需要知道反向代理服务器是谁就好了，www.baidu.com 就是我们的反向代理服务器，反向代理服务器会帮我们把请求转发到真实的服务器那里去。Nginx就是性能非常好的反向代理服务器，用来做负载均衡。 两者的区别在于代理的对象不一样：正向代理代理的对象是客户端，反向代理代理的对象是服务端 Nginx 使用反向代理，主要是使用location模块下的proxy_pass选项。 举个栗子： 123456789server &#123; listen 80; server_name www.baidu.com; access_log /usr/local/var/log/nginx/access.log main; error_log /usr/local/var/log/nginx/error.log error; location / &#123; proxy_pass http://222.222.220.1; &#125;&#125; 如此一来，当我们访问某度的时候，Nginx将我们的请求转发到真实服务器222.222.220.1那里，然后将我们需要的数据返回，而作为客户端的我们实际并不知道这幕后真正处理请求的服务器是谁。 Nginx的负载均衡 先简单说下负载均衡是干嘛的？再举个栗子：我们的小网站，刚开始就一台nginx服务器，后来，随着业务量增大，用户增多，一台服务器已经不够用了，我们就又多加了几台服务器。那么这几台服务器如何调度？如何均匀的提供访问？这就是负载均衡。 负载均衡的好处是可以集群多台机器一起工作，并且对外的IP 和 域名是一样的，外界看起来就好像一台机器一样。 Nginx 的 upstream目前支持 4 种方式的分配 1234567891)、轮询（默认） 每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 2)、weight 指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 2)、ip_hash 每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 3)、fair（第三方） 按后端服务器的响应时间来分配请求，响应时间短的优先分配。 4)、url_hash（第三方） 先来个最简单的 基于weight 权重的负载123456789101112131415upstream webservers&#123; server 192.168.33.11 weight=1 max_fails=2 fail_timeout=30s; server 192.168.33.12 weight=1 max_fails=2 fail_timeout=30s; server 192.168.33.13 weight=1 max_fails=2 fail_timeout=30s;&#125;server &#123; listen 80; server_name upstream.iyangyi.com; access_log /usr/local/var/log/access.log main; error_log /usr/local/var/log/nginx/error.log error; location / &#123; proxy_pass http://webservers; proxy_set_header X-Real-IP $remote_addr; &#125;&#125; 其中： max_fails: 允许请求失败的次数，默认为1。当超过最大次数时，返回proxy_next_upstream 模块定义的错误。 fail_timeout : 在经历了max_fails次失败后，暂停服务的时间。max_fails可以和fail_timeout一起使用，进行健康状态检查。 我们再来继续看剩下几个参数 : down和backup down 表示这台机器暂时不参与负载均衡。相当于注释掉了。 backup 表示这台机器是备用机器，是其他的机器不能用的时候，这台机器才会被使用，俗称备胎 O__O “… 我们继续来做实验，改一下，先把web1改成down，然后将web3改成backup: 12345upstream webservers&#123; server 192.168.33.11 down; server 192.168.33.12 weight=10 max_fails=2 fail_timeout=30s; server 192.168.33.13 backup;&#125; 重启下nginx，然后刷新下，不管怎么刷新，请求都将进入192.168.33.12。 接下来，我们将12上的 的服务停掉： sudo /usr/sbin/第二台服务 stop 然后，我们再刷新下网页，通过日志可以看到我们访问192.168.33.13； 基于 ip_hash 的负载 这种分配方式，每个请求按访问IP的hash结果分配，这样来自同一个IP的访客固定访问一个后端服务器，有效解决了动态网页存在的session共享问题。 动手看怎么操作： 123456upstream webservers&#123; ip_hash; server 192.168.33.11 weight=1 max_fails=2 fail_timeout=30s; server 192.168.33.12 weight=1 max_fails=2 fail_timeout=30s; server 192.168.33.13 down;&#125; 重启nginx，我们刷新，发现，再怎么刷192.168.33.11，都会进入固定的了。 我们讲12的权重该大一点： upstream webservers{ ip_hash; server 192.168.33.11 weight=1 max_fails=2 fail_timeout=30s; server 192.168.33.12 weight=2 max_fails=2 fail_timeout=30s; server 192.168.33.13 down; } 这样就会永远是192.168.33.12了。 我们试着把192.168.33.12服务关掉，再刷新，服务会全部进到192.168.33.11，已经切换过来了。 &gt; 注意 ip_hash 模式下，最好不要设置weight参数，因为你设置了，就相当于手动设置了，将会导致很多的流量分配不均匀。 ip_hash模式下, backup参数不可用，加了会报错，为啥呢？因为，本身我们的访问就是固定的了，其实，备用已经不管什么作用了。 参考 正向代理与反向代理 nginx的配置、虚拟主机、负载均衡和反向代理（2）]]></content>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql在MAC安装时忘记初始随机密码的解决办法]]></title>
    <url>%2F2016%2F08%2F06%2Fmysql-e5-9c-a8mac-e5-ae-89-e8-a3-85-e6-97-b6-e5-bf-98-e8-ae-b0-e5-88-9d-e5-a7-8b-e9-9a-8f-e6-9c-ba-e5-af-86-e7-a0-81-e7-9a-84-e8-a7-a3-e5-86-b3-e5-8a-%2F</url>
    <content type="text"><![CDATA[首先置空密码后可直接登录 1231. cd /usr/local/mysql/bin/2. sudo su ./mysqld_safe --skip-grant-tables &amp;3. 可能需要输入系统密码，正确输入后，现在的数据库密码将置为空 需要重新设置密码的话1update user set password=password(&quot;你的密码&quot;) where user=&quot;root&quot;; 如果是5.7的版本，那么应该是1update user set authentication_string=password(&quot;你的密码&quot;) where user=&quot;root&quot;;]]></content>
      <tags>
        <tag>DB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git Submodule 介绍和使用]]></title>
    <url>%2F2016%2F07%2F09%2Fgit-submodule--e6-bc-94-e7-a4-ba-e5-b1-81-e5-b1-81-e8-b8-a2%2F</url>
    <content type="text"><![CDATA[子模块初步 引用一段《Git权威指南》的话： 项目的版本库在某些情况虾需要引用其他版本库中的文件，例如公司积累了一套常用的函数库，被多个项目调用，显然这个函数库的代码不能直接放到某个项目的代码中，而是要独立为一个代码库，那么其他项目要调用公共函数库该如何处理呢？分别把公共函数库的文件拷贝到各自的项目中会造成冗余，丢弃了公共函数库的维护历史，这显然不是好的方法。 为现有的仓库添加submodule,以第三方库CodeMirror为示例，在当前仓库下执行: 查看当前仓库的状态，注意到：submodule 和 .gitmodules两个文件已经自动被staged。 1git status 其中.gitmodules保存了项目 URL 和你拉取到的本地子目录。如果有多个子module，则该文件会记录多个条目： 虽然 CodeMirror 是工作目录中的一个子目录，但 Git 还是会将它视作一个子模块。当你不在那个目录中时，Git 并不会跟踪它的内容， 而是将它看作该仓库中的一个特殊提交。 当你提交时，会看到类似下面的信息： 注意 CodeMirror 记录的 160000 模式。 这是 Git 中的一种特殊模式，它本质上意味着你是将一次提交记作一项目录记录的，而非将它记录成一个子目录或者一个文件。 最后推送至远端 1git push -u origin master 克隆一个带子模块的项目 你将得到了包含子项目的目录，但里面没有文件： 1git clone http://git.xhuabu.com/chenrunfa/UI-module.git 需要通过两个命令来初始化：git submodule init来初始化你的本地配置文件，git submodule update来从那个项目拉取所有数据并检出你上层项目里所列的合适的提交： 或者将这两个命令写成一句话：git submodule update --init --recursive 但最快的方法是，克隆时连同所有子项目一起拉下来： 1git clone --recursive http://git.xhuabu.com/chenrunfa/UI-module.git 删除子模块 删除子模块相对比较麻烦一些step by: 1. 删除git cache和物理文件夹 git rm -r --cached libs/ 删除.gitmodules rm .gitmodules,如果只是删除其中一个module，响应删除.gitmodules中对应条目即可 删除.git/config的submodule配置 源文件 提交更改 子模块深入 场景：某个工作中的项目需要包含并使用另一个项目。 也许是第三方库，或者你 独立开发的，用于多个父项目的库。现在你想要把它们当做两个独立的项目，同时又想在 一个项目中使用另一个。 克隆一个带有子模块的工程git submodule update --init --recursive 直接在主仓库里抓取与合并子模块 git submodule update --remote 或者只更新指定的子模块 - 进入子模块目录 git fetch git merge origin/master 在子模块上工作 当我们运行 git submodule update 从子模块仓库中抓取修改时，Git 将会获得这些改动并 更新子目录中的文件，但是会将子仓库留在一个称作 “游离的 HEAD” 的状态。 这意味着没有本 地工作分支（例如 “master”）跟踪改动。 所以你做的任何改动都不会被跟踪。 123git checkout master 进入子模块并检出相应的工作分支git submodule update --remote 从上游拉取数据git submodule update --remote --merge 从上游拉取数据并合并 发布子模块改动 如果我们在主项目中提交并推送但并不推送子模块上的改动，其他尝试检出我们修改的人会遇到 麻烦，因为他们无法得到依赖的子模块改动。 那些改动只存在于我们本地的拷贝中。 为了确保这不会发生，你可以让 Git 在推送到主项目前检查所有子模块是否已推送。 git push 命令接受可以设置为 check 或on-demand 的 –recurse-submodules 参数。 如果任何提交的子模块改动没有推送那么 check 选项会直接使push 操作失败。 - 提交主项目时自动检测子模块是否有未提交的改动 1git push --recurse-submodules=check 提交主项目时，尝试自动推送一改动的子模块 1git push --recurse-submodules=on-demand 案例分析 案例1：当前企业正在开发Project1项目，Project1中除了自身源代码外还包含了一个名为CommonLib的submodule，此时，项目开发中途加入了两位新的开发人员（员工A和员工B），参与开发最首要的就是先搭建环境并获取当前项目的代码，才能投入开发状态； A和B分别执行：git clone --recursive http://git.xhuabu.com/chenrunfa/UI-module.git 快速获取含有子模块的代码仓库。 案例2：在加入Project1项目的开发后，某一天，新员工A发现了CommonLib的代码中存在一个BUG或者是发现一个新的功能在其他项目也需要被用到，此时需要改动CommonLib中的源代码。 1A在修复了BUG之后，如果按照直接执行在Project1目录下执行git commit –am “相关描述”进行提交，对submodule发生的改变是没有任何作用的，因为这种提交方式只是提交当前Project1主工程的修改，对于submodule发生的改变不会进行提交。 正确的提交方式先进入CommonLib子模块的目录 12345678cd 子模块目录git commit –am “相关描述”（如果增删了文件，需要先做git add .操作）git rebase remotes/origin/master (同步服务器上的子模块代码)git push（提交子模块到远程库）cd 主工程目录git add 子模块目录（将子模块发生修改告诉你的主工程，这样他才能进行跟踪）git commit –am “相关描述”git push （提交主工程到远程库）完成以上流程的操作，才算正确的提交了子模块的修改，并让主工程跟踪跟新子模块。 案例3：新员工A在更新的CommonLib的子模块后，其他一起开发Project1项目的人员（包括了新员工B）都都需要同步更新CommonLib中的代码。 此时新员工B需要执行如下的几个操作来: 12345git pull （在主工程目录下同步更新代码git status（查看submodule是否发生了修改）git submodule update（如果子模块发生修改，需要执行此操作进行更新）如果子模块下还包含着子模块，最后一句的更新操作可以换成如下的命令：git submodule foreach git submodule update 实际上，submodule项目和它的父项目本质上是2个独立的git仓库。只是父项目存储了它依赖的submodule项目的版本号信息而已。如果其他同事更新了submodule，然后更新了父项目中依赖的版本号。你需要在git pull之后，调用 git submodule update来更新submodule信息。 有些时候你需要对submodule做一些修改，很常见的做法就是切到submodule的目录，然后做修改，然后commit和push。但是默认git submodule update并不会将submodule切到任何branch，所以，默认下submodule的HEAD是处于游离状态的(‘detached HEAD’state)。所以在修改前，记得一定要用git checkout master将当前的submodule分支切换到master，然后才能做修改和提交。 如果你不慎忘记切换到master分支，又做了提交，可以用cherry-pick命令挽救。具体做法如下： 1234git log （获取当前在游离分支提交的commitid）git checkout master（将HEAD从游离状态切换到 master 分支）git cherry-pick 游离分支上提交的commitID （将提交从游离分支放到master分支上）git push （将更新提交到远程版本库中） 以上案例来自：http://blog.csdn.net/d_clock/article/details/43730449 补充 iOS项目中通过submodule管理library的例子： http://blog.changyy.org/2012/12/ios-xcode-subprojects-git-submodule.html git submodule搭配上Android studio的modules來管理source： https://erttyy8821.gitbooks.io/android_memo/content/chapter1/SubmoduleAndModules.html git submodule详细使用说明: https://git-scm.com/book/zh/v2/Git-%E5%B7%A5%E5%85%B7-%E5%AD%90%E6%A8%A1%E5%9D%97]]></content>
      <tags>
        <tag>Git</tag>
        <tag>效率工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 下无法shutdown.sh Tomcat的解决办法]]></title>
    <url>%2F2016%2F06%2F21%2Flinux--e4-b8-8b-e6-97-a0-e6-b3-95shutdown-sh-tomcat-e7-9a-84-e8-a7-a3-e5-86-b3-e5-8a-9e-e6-b3-95%2F</url>
    <content type="text"><![CDATA[问题 tomcat6.0.41自带的./shutdown.sh经常无法停止进程 解决方案一： 查找到所有的tomcat进程 $ ps -ef | grep tomcat 然后逐一杀死它们 $ ps -9 pid 解决方案二： 前诉方案可能由于经常太多需要逐一杀死，太过繁琐。 $ kill -9 ps -ef|grep tomcat|awk &#39;{print $2}&#39; 解决方案三：（未经测试） $killall -9 tomcat]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux tail命令]]></title>
    <url>%2F2016%2F06%2F13%2Flinux-tail-e5-91-bd-e4-bb-a4%2F</url>
    <content type="text"><![CDATA[#命令使用 tail -f filename 说明：监视filename文件的尾部内容（默认10行，相当于增加参数 -n 10），刷新显示在屏幕上。退出，按下CTRL+C。 tail -n 20 filename 说明：显示filename最后20行。 tail -r -n 10 filename 说明：逆序显示filename最后10行。 #类似的命令跟tail功能相似的命令还有： cat 从第一行開始显示档案内容。 tac 从最后一行開始显示档案内容。 more 分页显示档案内容。 less 与 more 相似，但支持向前翻页 head 仅仅显示前面几行 tail 仅仅显示后面几行 n 带行号显示档案内容 od 以二进制方式显示档案内容]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jsp页面返回JSON数据]]></title>
    <url>%2F2016%2F06%2F11%2Fjsp-e9-a1-b5-e9-9d-a2-e8-bf-94-e5-9b-9ejson-e6-95-b0-e6-8d-ae%2F</url>
    <content type="text"><![CDATA[创建方法 123456789101112131415 public static String renderJson(HttpServletResponse response, String content)&#123; response.setContentType(&quot;application/json&quot;); response.setCharacterEncoding(&quot;UTF-8&quot;); response.setHeader(&quot;Cache-Control&quot;, &quot;no-cache&quot;); java.io.PrintWriter pw = null; try&#123; pw = response.getWriter(); pw.write(content); &#125;catch (Exception e)&#123; // &#125;finally&#123; pw.close(); &#125; return null; &#125; 在JSP中引入 123456789101112131415public static String renderJson(HttpServletResponse response, String content)&#123; response.setContentType(&quot;application/json&quot;); response.setCharacterEncoding(&quot;UTF-8&quot;); response.setHeader(&quot;Cache-Control&quot;, &quot;no-cache&quot;); java.io.PrintWriter pw = null; try&#123; pw = response.getWriter(); pw.write(content); &#125;catch (Exception e)&#123; // &#125;finally&#123; pw.close(); &#125; return null; &#125;]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS构造Map结构]]></title>
    <url>%2F2016%2F04%2F21%2Fjs-e6-9e-84-e9-80-a0map-e7-bb-93-e6-9e-84%2F</url>
    <content type="text"><![CDATA[直接上代码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126//定义map function Map() &#123; this.container = &#123;&#125;; &#125; //将key-value放入map中 Map.prototype.put = function(key,value)&#123; try&#123; if(key!=null &amp;&amp; key != &quot;&quot;) this.container[key] = value; &#125;catch(e)&#123; return e; &#125; &#125;; //根据key从map中取出对应的value Map.prototype.get = function(key)&#123; try&#123; return this.container[key]; &#125;catch(e)&#123; return e; &#125; &#125;; //判断map中是否包含指定的key Map.prototype.containsKey=function(key)&#123; try&#123; for(var p in this.container) &#123; if(this.p==key) return true; &#125; return false; &#125;catch(e)&#123; return e; &#125; &#125; //判断map中是否包含指定的value Map.prototype.containsValue = function(value)&#123; try&#123; for(var p in this.container) &#123; if(this.container[p] === value) return true; &#125; return false; &#125;catch(e)&#123; return e; &#125; &#125;; //删除map中指定的key Map.prototype.remove = function(key)&#123; try&#123; delete this.container[key]; &#125;catch(e)&#123; return e; &#125; &#125;; //清空map Map.prototype.clear = function()&#123; try&#123; delete this.container; this.container = &#123;&#125;; &#125;catch(e)&#123; return e; &#125; &#125;; //判断map是否为空 Map.prototype.isEmpty = function()&#123; if(this.keyArray().length==0) return true; else return false; &#125;; //获取map的大小 Map.prototype.size=function()&#123; return this.keyArray().length; &#125; //返回map中的key值数组 Map.prototype.keyArray=function()&#123; var keys=new Array(); for(var p in this.container) &#123; keys.push(p); &#125; return keys; &#125; //返回map中的value值数组 Map.prototype.valueArray=function()&#123; var values=new Array(); var keys=this.keyArray(); for(var i=0;i&lt;keys.length;i++) &#123; values.push(this.container[keys[i]]); &#125; return values; &#125;]]></content>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
</search>
